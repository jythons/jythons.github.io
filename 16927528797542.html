<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	5-小白学习大数据之Java实现UFO数据统计 - 奔跑在草原上的键盘手
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="奔跑在草原上的键盘手" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
					
					<h1><a href="index.html">奔跑在草原上的键盘手</a></h1>
					<p class="subtitle">工作学习中的点点滴滴。</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="_self" href="index.html">主页</a></li>
						
						  <li id=""><a target="_self" href="Java.html">Java</a></li>
						
						  <li id=""><a target="_self" href="nginx.html">Nginx</a></li>
						
						  <li id=""><a target="_self" href="Shell.html">Shell</a></li>
						
						  <li id=""><a target="_self" href="LVS.html">LVS</a></li>
						
						  <li id=""><a target="_self" href="Redis.html">Redis</a></li>
						
						  <li id=""><a target="_self" href="ES.html">ES</a></li>
						
						  <li id=""><a target="_self" href="MQ.html">MQ</a></li>
						
						  <li id=""><a target="_self" href="机器学习.html">机器学习</a></li>
						
						  <li id=""><a target="_self" href="大数据.html">大数据</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="jythons.github.io" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:jythons@sina.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">

	<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
		<h1 class="title" itemprop="name">5-小白学习大数据之Java实现UFO数据统计</h1>
		<div class="entry-content" itemprop="articleBody">
			<p>上一个记录了使用Python实现UFO数据的统计案例，下面使用Java来开发一下UFO数据的统计。</p>
<span id="more"></span><!-- more -->
<h2><a id="1%E3%80%81%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true" href="#1%E3%80%81%E5%88%86%E6%9E%90"><span class="octicon octicon-link"></span></a>1、分析</h2>
<p>在使用Python开发MapReduce的时候，我们对数据进行了过滤，不满足6条的数据忽略，那么当我们需要使用同一个结果集进行多个维度统计的时候，是否可以将上面过滤数据的部分抽出来共用呢。<br />
答案是肯定的，我们可以使用<code>org.apache.hadoop.mapred.lib.ChainMapper</code>来解决这个问题。ChainMapper类可以顺序执行多个mapper，而且最后的mapper输出会传递给reducer。 ChainMapper不仅适用于这种数据清理，而且在分析特定作业时，先通过它执行多个map型任务 再应用reducer的做法也很常见。<br />
这种做法需要编写一个校验mapper，它可用于将来所有的字段分析作业。校验mapper会丢弃 错误记录行，只将有效的行传入实际的业务逻辑mapper。这样的话，目前的业务逻辑mapper就可 以专注于分析数据而不用担心粗粒度的校验。</p>
<h2><a id="2%E3%80%81%E4%BD%BF%E7%94%A8chainmapper%E8%BF%9B%E8%A1%8C%E5%AD%97%E6%AE%B5%E9%AA%8C%E8%AF%81%E3%80%81%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true" href="#2%E3%80%81%E4%BD%BF%E7%94%A8chainmapper%E8%BF%9B%E8%A1%8C%E5%AD%97%E6%AE%B5%E9%AA%8C%E8%AF%81%E3%80%81%E5%88%86%E6%9E%90"><span class="octicon octicon-link"></span></a>2、使用ChainMapper进行字段验证、分析</h2>
<ol>
<li>在<code>UFORecordValidationMapper.java</code>文件中创建如下类：</li>
</ol>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;

public class UFORecordValidationMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, LongWritable, Text&gt; {
    @Override
    public void map(LongWritable key, Text value, OutputCollector&lt;LongWritable, Text&gt; output, Reporter reporter) throws IOException {
        String line = value.toString();
        if (validate(line))
            output.collect(key, value);
    }

    private boolean validate(String str) {
        String[] parts = str.split(&quot;\t&quot;);
        if (parts.length != 6)
            return false;
        return true;
    }
}
</code></pre>
<ol start="2">
<li>在<code>UFOLocation.java</code>文件中创建如下类：</li>
</ol>
<pre><code class="language-java">
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapred.lib.ChainMapper;
import org.apache.hadoop.mapred.lib.LongSumReducer;

import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class UFOLocation {
    public static class MapClass extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
        private final static LongWritable one = new LongWritable(1);
        private static Pattern locationPattern = Pattern.compile(&quot;[a-zA-Z]{2}[^a-zA-Z]*$&quot;);

        public void map(LongWritable key, Text value, OutputCollector&lt;Text, LongWritable&gt; output, Reporter reporter) throws IOException {
            String line = value.toString();
            String[] fields = line.split(&quot;\t&quot;);
            String location = fields[2].trim();
            if (location.length() &gt;= 2) {
                Matcher matcher = locationPattern.matcher(location);
                if (matcher.find()) {
                    int start = matcher.start();
                    String state = location.substring(start, start + 2);
                    output.collect(new Text(state.toUpperCase()), one);
                }

            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration config = new Configuration();
        JobConf conf = new JobConf(config, UFOLocation.class);
        conf.setJobName(&quot;UFOLocation&quot;);
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(LongWritable.class);

        JobConf mapconf1 = new JobConf(false);
        ChainMapper.addMapper(conf, UFORecordValidationMapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapconf1);

        JobConf mapconf2 = new JobConf(false);
        ChainMapper.addMapper(conf, MapClass.class, LongWritable.class, Text.class, Text.class, LongWritable.class, true, mapconf2);
        conf.setMapperClass(ChainMapper.class);

        conf.setCombinerClass(LongSumReducer.class);
        conf.setReducerClass(LongSumReducer.class);

        FileInputFormat.setInputPaths(conf, args[0]);
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        JobClient.runJob(conf);
    }
}

</code></pre>
<ol start="3">
<li>编译上面两个文件</li>
</ol>
<pre><code class="language-shell">&gt; javac UFORecordValidationMapper.java UFOLocation.java 
</code></pre>
<ol start="4">
<li>将上述两个类文件打包为ufo1.jar文件并提交作业至Hadoop</li>
</ol>
<pre><code class="language-shell">&gt; jar cvf ufo1.jar *.class
</code></pre>
<ol start="5">
<li>将输出文件拷贝至本地文件系统并检查它</li>
</ol>
<pre><code class="language-shell">&gt; hadoop jar ./ufo1.jar UFOLocation /user/root/ufo.tsv output
&gt; hdfs dfs -cat /user/root/output/part-00000
  AB      286
  AD      6
  AE      7
  AI      6
  AK      234
  AL      548
  AM      22
  ...
</code></pre>
<p>本作业的驱动程序发生的变化最大。之前的驱动配置中仅包含一个map类，而本作业的驱动配置要多次调用ChainMapper类。<br />
在驱动中多次调用ChainMapper类的一般模式是，为每个mapper新建一个配置对象，然后将mapper添加到ChainMapper类，同时指定输入和输出位置，并引用整个作业的配置对象。<br />
请注意，上述两个mapper的参数略有不同。它们都输入LongWritable类型的键及Text类 型的值。区别在于输出的数据类型:UFORecordValidationMapper输出LongWritable类型的 键及Text类型的值，而UFOLocationMapper则相反，输出Text类型的键及LongWritable类型 的值。<br />
重要的是，要保证mapper链条末端的UFOLocationMapper的输入与reduce类(LongSum- Reducer)的输入类型相匹配。在使用ChainMapper类时，只要符合下列条件，链条中的mapper可以有不同的输入和输出:</p>
<ul>
<li>除最后一个mapper外，链条中每个map的输出与下一个mapper的输入相匹配;</li>
<li>对最后一个mapper而言，其输出与reducer输入相匹配。</li>
</ul>
<h2><a id="3%E3%80%81%E6%94%B9%E8%BF%9B%E8%BE%93%E5%87%BA%E5%9C%B0%E7%82%B9" class="anchor" aria-hidden="true" href="#3%E3%80%81%E6%94%B9%E8%BF%9B%E8%BE%93%E5%87%BA%E5%9C%B0%E7%82%B9"><span class="octicon octicon-link"></span></a>3、改进输出地点</h2>
<h3><a id="3-1%E3%80%81%E4%B8%8D%E8%B6%B3" class="anchor" aria-hidden="true" href="#3-1%E3%80%81%E4%B8%8D%E8%B6%B3"><span class="octicon octicon-link"></span></a>3.1、不足</h3>
<p>上面的地点取首字母来表示地点并不是完全可行，在人工分析源文件之后，许多数据问题浮出水面。</p>
<ul>
<li>州名的大写缩写不一致。</li>
<li>大量的目击事件并非发生在美国，尽管它们可能遵守类似(城市，地区)的格式，但是它们的缩写并不在我们预计的50个地区缩写之内。</li>
<li>有些字段根本不遵守(城市，地区)这样的规则，但仍会被正则表达式采集到。</li>
</ul>
<p>我们需要对结果进行过滤，最好是将美国记录标准化为正确的州名输出，并将其余数据划分 为一个范围更宽泛的大类。<br />
为了执行这个任务，需要在mapper中添加一些内容，让它明白什么是有效的美国州名缩写。 当然，我们可以将其硬编码到mapper中，但这似乎不是正确的做法。虽然现在我们计划将所有非 美国的目击事件视为一类，但我们今后还可能对该类进行扩展，比如按国家进行划分。如果将州 名缩写硬编码到mapper，那就需要每次重新编译我们的mapper。</p>
<h3><a id="3-2%E3%80%81%E4%BD%BF%E7%94%A8-distributed-cache%E6%94%B9%E8%BF%9B%E5%9C%B0%E7%82%B9%E8%BE%93%E5%87%BA" class="anchor" aria-hidden="true" href="#3-2%E3%80%81%E4%BD%BF%E7%94%A8-distributed-cache%E6%94%B9%E8%BF%9B%E5%9C%B0%E7%82%B9%E8%BE%93%E5%87%BA"><span class="octicon octicon-link"></span></a>3.2、使用 Distributed Cache 改进地点输出</h3>
<p>我们将整理好的洲名和首字母对应的关系文件保存到HDFS上。</p>
<pre><code class="language-shell">hdfs dfs -put states.txt(本地) states.txt（hdfs）
</code></pre>
<p>文件使用：首字母+制表符+州名的格式，格式如下：</p>
<pre><code>AL	Alabama
AK	Alaska
AZ	Arizona
AR	Arkansas
CA	California
CO	Colorado
CT	Connecticut 
...
</code></pre>
<p>修改<code>UFOLocation.java</code>文件代码如下：</p>
<pre><code class="language-java">//package ufo2;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapred.lib.ChainMapper;
import org.apache.hadoop.mapred.lib.LongSumReducer;
import org.apache.hadoop.mapreduce.Job;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * @author jiaoyang
 * @title: UFOLocation
 * @projectName java
 * @description: TODO
 * @date 2023/8/2215:50
 */
public class UFOLocation {
    public static class MapClass extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
        private final static LongWritable one = new LongWritable(1);
        private static Pattern locationPattern = Pattern.compile(&quot;[a-zA-Z]{2}[^a-zA-Z]*$&quot;);
        private Map&lt;String, String&gt; stateNames;

        @Override
        public void configure(JobConf job) {
            try {
                Path[] cacheFiles = JobContextImpl.getLocalCacheFiles(job);
                String statesTxt = cacheFiles[0].toString().replace(&quot;file:&quot;, &quot;&quot;).trim();
                System.out.println(statesTxt);
                setupStateMap(statesTxt);
            } catch (IOException e) {
                System.err.println(&quot;Error reading state file.&quot;);
                e.printStackTrace();
//                System.err.println(Arrays.toString(e.getStackTrace()));
                System.exit(1);
            }
        }

        private void setupStateMap(String filename) throws IOException {
            Map&lt;String, String&gt; states = new HashMap&lt;String, String&gt;();
            try (BufferedReader reader = new BufferedReader(new FileReader(filename))) {
                String line = reader.readLine();
                while (line != null) {
                    String[] split = line.split(&quot;\t&quot;);
                    states.put(split[0], split[1]);
                    line = reader.readLine();
                }
            }
            stateNames = states;
        }

        public void map(LongWritable key, Text value, OutputCollector&lt;Text, LongWritable&gt; output, Reporter reporter) throws IOException {
            String line = value.toString();
            String[] fields = line.split(&quot;\t&quot;);
            String location = fields[2].trim();
            if (location.length() &gt;= 2) {
                Matcher matcher = locationPattern.matcher(location);
                if (matcher.find()) {
                    int start = matcher.start();
                    String state = location.substring(start, start + 2);
//                    output.collect(new Text(state.toUpperCase()), one);
                    output.collect(new Text(lookupState(state.toUpperCase())), one);
                }

            }
        }

        private String lookupState( String state)
        {
            String fullName = stateNames.get(state) ;
            return fullName == null? &quot;Other&quot;: fullName ;
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration config = new Configuration();
        JobConf conf = new JobConf(config, UFOLocation.class);

        conf.setJobName(&quot;UFOLocation&quot;);
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(LongWritable.class);

        // 这个方法，在2.2版本已弃用DistributedCache.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf) ;
        Job.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf);

        JobConf mapconf1 = new JobConf(false);
        ChainMapper.addMapper(conf, UFORecordValidationMapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapconf1);

        JobConf mapconf2 = new JobConf(false);
        ChainMapper.addMapper(conf, MapClass.class, LongWritable.class, Text.class, Text.class, LongWritable.class, true, mapconf2);
        conf.setMapperClass(ChainMapper.class);

        conf.setCombinerClass(LongSumReducer.class);
        conf.setReducerClass(LongSumReducer.class);

        FileInputFormat.setInputPaths(conf, args[0]);
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        JobClient.runJob(conf);
    }
}

</code></pre>
<p>上面的代码在<code>main</code>方法中添加了:</p>
<pre><code class="language-java">Job.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf);
</code></pre>
<p>用于将文件分发到各个节点。<br />
在<code>MapClass</code>内部类中添加了重写了<code>MapReduceBase</code>类的<code>configure</code>方法，该方法使用map方法将州名缩写与全称关联起来。<code>configure</code>方法在任务启动时被调用，其默认实现不执行任何操作。</p>
<blockquote>
<p>写上面代码的时候，发现了一个问题，cacheFiles[0].toString() 返回的文件路径前面多了一个：“file:”，导致后面读取文件的时候报错，这里我直接操作字符串给替换掉了，如果有人有好的办法请多指教。</p>
</blockquote>
<p>将修改好的代码编译打包，执行后获得一下结果：</p>
<pre><code>[root@5b49e23aa62e ufo2]# hdfs dfs -cat output_ufo2/part-00000
Alabama 548
Alaska  234
Arizona 2097
Arkansas        534
California      7679
Colorado        1457
Connecticut     608
Delaware        127
...
</code></pre>
<h2><a id="4%E3%80%81%E5%AE%9E%E7%8E%B0%E8%AE%A1%E6%95%B0%E5%99%A8%EF%BC%8C%E7%BB%9F%E8%AE%A1%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E6%95%B0%E9%87%8F" class="anchor" aria-hidden="true" href="#4%E3%80%81%E5%AE%9E%E7%8E%B0%E8%AE%A1%E6%95%B0%E5%99%A8%EF%BC%8C%E7%BB%9F%E8%AE%A1%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E6%95%B0%E9%87%8F"><span class="octicon octicon-link"></span></a>4、实现计数器，统计被忽略的数据数量</h2>
<p>上面的代码，统计了每个州的数据汇总信息，但是我们在汇总的时候，过滤掉了部分不满足统计条件的数据，这部分数据量不知道是多少，下面我们使用自定义的计数器，来统计一下这部分数据数量。
修改<code>UFORecordValidationMapper.java</code>文件，改造如下：</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;

public class UFORecordValidationMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, LongWritable, Text&gt; {
    public enum LineCounters {
        BAD_LINES,
        TOO_MANY_TABS,
        TOO_FEW_TABS
    }

    @Override
    public void map(LongWritable key, Text value, OutputCollector&lt;LongWritable, Text&gt; output, Reporter reporter) throws IOException {
        String line = value.toString();
        if (validate(line, reporter))
            output.collect(key, value);
    }

    private boolean validate(String str, Reporter reporter) {
        String[] parts = str.split(&quot;\t&quot;);
        if (parts.length != 6) {
            if (parts.length &lt; 6) {
                reporter.incrCounter(LineCounters.TOO_FEW_TABS, 1);
            } else {
                reporter.incrCounter(LineCounters.TOO_MANY_TABS, 1);
            }
            reporter.incrCounter(LineCounters.BAD_LINES, 1);
            if ((reporter.getCounter(LineCounters.BAD_LINES).getCounter() % 10) == 0) {
                reporter.setStatus(&quot;Got 10 bad lines.&quot;);
                System.err.println(&quot;Read another 10 bad lines.&quot;);
            }
            return false;
        }

        return true;
    }
}

</code></pre>
<p>编译运行：</p>
<pre><code class="language-shell">&gt; hadoop jar ./ufo2.jar UFOLocation /user/root/ufo.tsv output_ufo3
...
UFORecordValidationMapper$LineCounters
                BAD_LINES=326
                TOO_FEW_TABS=2
                TOO_MANY_TABS=324
...
</code></pre>
<blockquote>
<p>假如你用的是Hadoop context对象API，就要用Context.getCounter(). increment()方法访问计数器。</p>
</blockquote>

		</div>
	</article>
	<div class="share-comment">
	 

	  

	  

	</div>
</div>        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>



  













<script src="asset/prism.js"></script>


<style type="text/css">
figure{margin: 1em 0;padding: 0;}
  figcaption{text-align:center;}

/* PrismJS 1.14.0
https://prismjs.com/download.html#themes=prism-coy&languages=markup+css+clike+javascript */
/**
 * prism.js Coy theme for JavaScript, CoffeeScript, CSS and HTML
 * Based on https://github.com/tshedor/workshop-wp-theme (Example: http://workshop.kansan.com/category/sessions/basics or http://workshop.timshedor.com/category/sessions/basics);
 * @author Tim  Shedor
 */

code[class*="language-"],
pre[class*="language-"] {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  position: relative;
  margin: .5em 0;
  overflow: visible;
  padding: 0;
}
pre[class*="language-"]>code {
  position: relative;
  border-left: 10px solid #358ccb;
  box-shadow: -1px 0px 0px 0px #358ccb, 0px 0px 0px 1px #dfdfdf;
  background-color: #fdfdfd;
  background-image: linear-gradient(transparent 50%, rgba(69, 142, 209, 0.04) 50%);
  background-size: 3em 3em;
  background-origin: content-box;
  background-attachment: local;
}

code[class*="language"] {
  max-height: inherit;
  height: inherit;
  padding: 0 1em;
  display: block;
  overflow: auto;
}

/* Margin bottom to accomodate shadow */
:not(pre) > code[class*="language-"],
pre[class*="language-"] {
  background-color: #fdfdfd;
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
  margin-bottom: 1em;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  position: relative;
  padding: .2em;
  border-radius: 0.3em;
  color: #c92c2c;
  border: 1px solid rgba(0, 0, 0, 0.1);
  display: inline;
  white-space: normal;
}

pre[class*="language-"]:before,
pre[class*="language-"]:after {
  content: '';
  z-index: -2;
  display: block;
  position: absolute;
  bottom: 0.75em;
  left: 0.18em;
  width: 40%;
  height: 20%;
  max-height: 13em;
  box-shadow: 0px 13px 8px #979797;
  -webkit-transform: rotate(-2deg);
  -moz-transform: rotate(-2deg);
  -ms-transform: rotate(-2deg);
  -o-transform: rotate(-2deg);
  transform: rotate(-2deg);
}

:not(pre) > code[class*="language-"]:after,
pre[class*="language-"]:after {
  right: 0.75em;
  left: auto;
  -webkit-transform: rotate(2deg);
  -moz-transform: rotate(2deg);
  -ms-transform: rotate(2deg);
  -o-transform: rotate(2deg);
  transform: rotate(2deg);
}

.token.comment,
.token.block-comment,
.token.prolog,
.token.doctype,
.token.cdata {
  color: #7D8B99;
}

.token.punctuation {
  color: #5F6364;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.function-name,
.token.constant,
.token.symbol,
.token.deleted {
  color: #c92c2c;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.function,
.token.builtin,
.token.inserted {
  color: #2f9c0a;
}

.token.operator,
.token.entity,
.token.url,
.token.variable {
  color: #a67f59;
  background: rgba(255, 255, 255, 0.5);
}

.token.atrule,
.token.attr-value,
.token.keyword,
.token.class-name {
  color: #1990b8;
}

.token.regex,
.token.important {
  color: #e90;
}

.language-css .token.string,
.style .token.string {
  color: #a67f59;
  background: rgba(255, 255, 255, 0.5);
}

.token.important {
  font-weight: normal;
}

.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}

.token.entity {
  cursor: help;
}

.namespace {
  opacity: .7;
}

@media screen and (max-width: 767px) {
  pre[class*="language-"]:before,
  pre[class*="language-"]:after {
    bottom: 14px;
    box-shadow: none;
  }

}

/* Plugin styles */
.token.tab:not(:empty):before,
.token.cr:before,
.token.lf:before {
  color: #e0d7d1;
}

/* Plugin styles: Line Numbers */
pre[class*="language-"].line-numbers.line-numbers {
  padding-left: 0;
}

pre[class*="language-"].line-numbers.line-numbers code {
  padding-left: 3.8em;
}

pre[class*="language-"].line-numbers.line-numbers .line-numbers-rows {
  left: 0;
}

/* Plugin styles: Line Highlight */
pre[class*="language-"][data-line] {
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 0;
}
pre[data-line] code {
  position: relative;
  padding-left: 4em;
}
pre .line-highlight {
  margin-top: 0;
}

pre[class*="language-"].line-numbers {
    position: relative;
    padding-left: 3.8em;
    counter-reset: linenumber;
}

pre[class*="language-"].line-numbers > code {
    position: relative;
    white-space: inherit;
}

.line-numbers .line-numbers-rows {
    position: absolute;
    pointer-events: none;
    top: 0;
    font-size: 100%;
    left: -3.8em;
    width: 3em; /* works for line-numbers below 1000 lines */
    letter-spacing: -1px;
    border-right: 1px solid #999;

    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;

}

    .line-numbers-rows > span {
        pointer-events: none;
        display: block;
        counter-increment: linenumber;
    }

        .line-numbers-rows > span:before {
            content: counter(linenumber);
            color: #999;
            display: block;
            padding-right: 0.8em;
            text-align: right;
        }

</style>
  
    


</body>
</html>