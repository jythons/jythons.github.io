<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[奔跑在草原上的键盘手]]></title>
  <link href="jythons.github.io/atom.xml" rel="self"/>
  <link href="jythons.github.io/"/>
  <updated>2023-08-23T14:43:46+08:00</updated>
  <id>jythons.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[5-小白学习大数据之Java实现UFO数据统计]]></title>
    <link href="jythons.github.io/16927528797542.html"/>
    <updated>2023-08-23T09:07:59+08:00</updated>
    <id>jythons.github.io/16927528797542.html</id>
    <content type="html"><![CDATA[
<p>上一个记录了使用Python实现UFO数据的统计案例，下面使用Java来开发一下UFO数据的统计。</p>
<span id="more"></span><!-- more -->
<h2><a id="1%E3%80%81%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true" href="#1%E3%80%81%E5%88%86%E6%9E%90"><span class="octicon octicon-link"></span></a>1、分析</h2>
<p>在使用Python开发MapReduce的时候，我们对数据进行了过滤，不满足6条的数据忽略，那么当我们需要使用同一个结果集进行多个维度统计的时候，是否可以将上面过滤数据的部分抽出来共用呢。<br />
答案是肯定的，我们可以使用<code>org.apache.hadoop.mapred.lib.ChainMapper</code>来解决这个问题。ChainMapper类可以顺序执行多个mapper，而且最后的mapper输出会传递给reducer。 ChainMapper不仅适用于这种数据清理，而且在分析特定作业时，先通过它执行多个map型任务 再应用reducer的做法也很常见。<br />
这种做法需要编写一个校验mapper，它可用于将来所有的字段分析作业。校验mapper会丢弃 错误记录行，只将有效的行传入实际的业务逻辑mapper。这样的话，目前的业务逻辑mapper就可 以专注于分析数据而不用担心粗粒度的校验。</p>
<h2><a id="2%E3%80%81%E4%BD%BF%E7%94%A8chainmapper%E8%BF%9B%E8%A1%8C%E5%AD%97%E6%AE%B5%E9%AA%8C%E8%AF%81%E3%80%81%E5%88%86%E6%9E%90" class="anchor" aria-hidden="true" href="#2%E3%80%81%E4%BD%BF%E7%94%A8chainmapper%E8%BF%9B%E8%A1%8C%E5%AD%97%E6%AE%B5%E9%AA%8C%E8%AF%81%E3%80%81%E5%88%86%E6%9E%90"><span class="octicon octicon-link"></span></a>2、使用ChainMapper进行字段验证、分析</h2>
<ol>
<li>在<code>UFORecordValidationMapper.java</code>文件中创建如下类：</li>
</ol>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;

public class UFORecordValidationMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, LongWritable, Text&gt; {
    @Override
    public void map(LongWritable key, Text value, OutputCollector&lt;LongWritable, Text&gt; output, Reporter reporter) throws IOException {
        String line = value.toString();
        if (validate(line))
            output.collect(key, value);
    }

    private boolean validate(String str) {
        String[] parts = str.split(&quot;\t&quot;);
        if (parts.length != 6)
            return false;
        return true;
    }
}
</code></pre>
<ol start="2">
<li>在<code>UFOLocation.java</code>文件中创建如下类：</li>
</ol>
<pre><code class="language-java">
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapred.lib.ChainMapper;
import org.apache.hadoop.mapred.lib.LongSumReducer;

import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class UFOLocation {
    public static class MapClass extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
        private final static LongWritable one = new LongWritable(1);
        private static Pattern locationPattern = Pattern.compile(&quot;[a-zA-Z]{2}[^a-zA-Z]*$&quot;);

        public void map(LongWritable key, Text value, OutputCollector&lt;Text, LongWritable&gt; output, Reporter reporter) throws IOException {
            String line = value.toString();
            String[] fields = line.split(&quot;\t&quot;);
            String location = fields[2].trim();
            if (location.length() &gt;= 2) {
                Matcher matcher = locationPattern.matcher(location);
                if (matcher.find()) {
                    int start = matcher.start();
                    String state = location.substring(start, start + 2);
                    output.collect(new Text(state.toUpperCase()), one);
                }

            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration config = new Configuration();
        JobConf conf = new JobConf(config, UFOLocation.class);
        conf.setJobName(&quot;UFOLocation&quot;);
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(LongWritable.class);

        JobConf mapconf1 = new JobConf(false);
        ChainMapper.addMapper(conf, UFORecordValidationMapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapconf1);

        JobConf mapconf2 = new JobConf(false);
        ChainMapper.addMapper(conf, MapClass.class, LongWritable.class, Text.class, Text.class, LongWritable.class, true, mapconf2);
        conf.setMapperClass(ChainMapper.class);

        conf.setCombinerClass(LongSumReducer.class);
        conf.setReducerClass(LongSumReducer.class);

        FileInputFormat.setInputPaths(conf, args[0]);
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        JobClient.runJob(conf);
    }
}

</code></pre>
<ol start="3">
<li>编译上面两个文件</li>
</ol>
<pre><code class="language-shell">&gt; javac UFORecordValidationMapper.java UFOLocation.java 
</code></pre>
<ol start="4">
<li>将上述两个类文件打包为ufo1.jar文件并提交作业至Hadoop</li>
</ol>
<pre><code class="language-shell">&gt; jar cvf ufo1.jar *.class
</code></pre>
<ol start="5">
<li>将输出文件拷贝至本地文件系统并检查它</li>
</ol>
<pre><code class="language-shell">&gt; hadoop jar ./ufo1.jar UFOLocation /user/root/ufo.tsv output
&gt; hdfs dfs -cat /user/root/output/part-00000
  AB      286
  AD      6
  AE      7
  AI      6
  AK      234
  AL      548
  AM      22
  ...
</code></pre>
<p>本作业的驱动程序发生的变化最大。之前的驱动配置中仅包含一个map类，而本作业的驱动配置要多次调用ChainMapper类。<br />
在驱动中多次调用ChainMapper类的一般模式是，为每个mapper新建一个配置对象，然后将mapper添加到ChainMapper类，同时指定输入和输出位置，并引用整个作业的配置对象。<br />
请注意，上述两个mapper的参数略有不同。它们都输入LongWritable类型的键及Text类 型的值。区别在于输出的数据类型:UFORecordValidationMapper输出LongWritable类型的 键及Text类型的值，而UFOLocationMapper则相反，输出Text类型的键及LongWritable类型 的值。<br />
重要的是，要保证mapper链条末端的UFOLocationMapper的输入与reduce类(LongSum- Reducer)的输入类型相匹配。在使用ChainMapper类时，只要符合下列条件，链条中的mapper可以有不同的输入和输出:</p>
<ul>
<li>除最后一个mapper外，链条中每个map的输出与下一个mapper的输入相匹配;</li>
<li>对最后一个mapper而言，其输出与reducer输入相匹配。</li>
</ul>
<h2><a id="3%E3%80%81%E6%94%B9%E8%BF%9B%E8%BE%93%E5%87%BA%E5%9C%B0%E7%82%B9" class="anchor" aria-hidden="true" href="#3%E3%80%81%E6%94%B9%E8%BF%9B%E8%BE%93%E5%87%BA%E5%9C%B0%E7%82%B9"><span class="octicon octicon-link"></span></a>3、改进输出地点</h2>
<h3><a id="3-1%E3%80%81%E4%B8%8D%E8%B6%B3" class="anchor" aria-hidden="true" href="#3-1%E3%80%81%E4%B8%8D%E8%B6%B3"><span class="octicon octicon-link"></span></a>3.1、不足</h3>
<p>上面的地点取首字母来表示地点并不是完全可行，在人工分析源文件之后，许多数据问题浮出水面。</p>
<ul>
<li>州名的大写缩写不一致。</li>
<li>大量的目击事件并非发生在美国，尽管它们可能遵守类似(城市，地区)的格式，但是它们的缩写并不在我们预计的50个地区缩写之内。</li>
<li>有些字段根本不遵守(城市，地区)这样的规则，但仍会被正则表达式采集到。</li>
</ul>
<p>我们需要对结果进行过滤，最好是将美国记录标准化为正确的州名输出，并将其余数据划分 为一个范围更宽泛的大类。<br />
为了执行这个任务，需要在mapper中添加一些内容，让它明白什么是有效的美国州名缩写。 当然，我们可以将其硬编码到mapper中，但这似乎不是正确的做法。虽然现在我们计划将所有非 美国的目击事件视为一类，但我们今后还可能对该类进行扩展，比如按国家进行划分。如果将州 名缩写硬编码到mapper，那就需要每次重新编译我们的mapper。</p>
<h3><a id="3-2%E3%80%81%E4%BD%BF%E7%94%A8-distributed-cache%E6%94%B9%E8%BF%9B%E5%9C%B0%E7%82%B9%E8%BE%93%E5%87%BA" class="anchor" aria-hidden="true" href="#3-2%E3%80%81%E4%BD%BF%E7%94%A8-distributed-cache%E6%94%B9%E8%BF%9B%E5%9C%B0%E7%82%B9%E8%BE%93%E5%87%BA"><span class="octicon octicon-link"></span></a>3.2、使用 Distributed Cache 改进地点输出</h3>
<p>我们将整理好的洲名和首字母对应的关系文件保存到HDFS上。</p>
<pre><code class="language-shell">hdfs dfs -put states.txt(本地) states.txt（hdfs）
</code></pre>
<p>文件使用：首字母+制表符+州名的格式，格式如下：</p>
<pre><code>AL	Alabama
AK	Alaska
AZ	Arizona
AR	Arkansas
CA	California
CO	Colorado
CT	Connecticut 
...
</code></pre>
<p>修改<code>UFOLocation.java</code>文件代码如下：</p>
<pre><code class="language-java">//package ufo2;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.mapred.lib.ChainMapper;
import org.apache.hadoop.mapred.lib.LongSumReducer;
import org.apache.hadoop.mapreduce.Job;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * @author jiaoyang
 * @title: UFOLocation
 * @projectName java
 * @description: TODO
 * @date 2023/8/2215:50
 */
public class UFOLocation {
    public static class MapClass extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, LongWritable&gt; {
        private final static LongWritable one = new LongWritable(1);
        private static Pattern locationPattern = Pattern.compile(&quot;[a-zA-Z]{2}[^a-zA-Z]*$&quot;);
        private Map&lt;String, String&gt; stateNames;

        @Override
        public void configure(JobConf job) {
            try {
                Path[] cacheFiles = JobContextImpl.getLocalCacheFiles(job);
                String statesTxt = cacheFiles[0].toString().replace(&quot;file:&quot;, &quot;&quot;).trim();
                System.out.println(statesTxt);
                setupStateMap(statesTxt);
            } catch (IOException e) {
                System.err.println(&quot;Error reading state file.&quot;);
                e.printStackTrace();
//                System.err.println(Arrays.toString(e.getStackTrace()));
                System.exit(1);
            }
        }

        private void setupStateMap(String filename) throws IOException {
            Map&lt;String, String&gt; states = new HashMap&lt;String, String&gt;();
            try (BufferedReader reader = new BufferedReader(new FileReader(filename))) {
                String line = reader.readLine();
                while (line != null) {
                    String[] split = line.split(&quot;\t&quot;);
                    states.put(split[0], split[1]);
                    line = reader.readLine();
                }
            }
            stateNames = states;
        }

        public void map(LongWritable key, Text value, OutputCollector&lt;Text, LongWritable&gt; output, Reporter reporter) throws IOException {
            String line = value.toString();
            String[] fields = line.split(&quot;\t&quot;);
            String location = fields[2].trim();
            if (location.length() &gt;= 2) {
                Matcher matcher = locationPattern.matcher(location);
                if (matcher.find()) {
                    int start = matcher.start();
                    String state = location.substring(start, start + 2);
//                    output.collect(new Text(state.toUpperCase()), one);
                    output.collect(new Text(lookupState(state.toUpperCase())), one);
                }

            }
        }

        private String lookupState( String state)
        {
            String fullName = stateNames.get(state) ;
            return fullName == null? &quot;Other&quot;: fullName ;
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration config = new Configuration();
        JobConf conf = new JobConf(config, UFOLocation.class);

        conf.setJobName(&quot;UFOLocation&quot;);
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(LongWritable.class);

        // 这个方法，在2.2版本已弃用DistributedCache.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf) ;
        Job.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf);

        JobConf mapconf1 = new JobConf(false);
        ChainMapper.addMapper(conf, UFORecordValidationMapper.class, LongWritable.class, Text.class, LongWritable.class, Text.class, true, mapconf1);

        JobConf mapconf2 = new JobConf(false);
        ChainMapper.addMapper(conf, MapClass.class, LongWritable.class, Text.class, Text.class, LongWritable.class, true, mapconf2);
        conf.setMapperClass(ChainMapper.class);

        conf.setCombinerClass(LongSumReducer.class);
        conf.setReducerClass(LongSumReducer.class);

        FileInputFormat.setInputPaths(conf, args[0]);
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));

        JobClient.runJob(conf);
    }
}

</code></pre>
<p>上面的代码在<code>main</code>方法中添加了:</p>
<pre><code class="language-java">Job.addCacheFile(new URI(&quot;/user/root/states.txt&quot;), conf);
</code></pre>
<p>用于将文件分发到各个节点。<br />
在<code>MapClass</code>内部类中添加了重写了<code>MapReduceBase</code>类的<code>configure</code>方法，该方法使用map方法将州名缩写与全称关联起来。<code>configure</code>方法在任务启动时被调用，其默认实现不执行任何操作。</p>
<blockquote>
<p>写上面代码的时候，发现了一个问题，cacheFiles[0].toString() 返回的文件路径前面多了一个：“file:”，导致后面读取文件的时候报错，这里我直接操作字符串给替换掉了，如果有人有好的办法请多指教。</p>
</blockquote>
<p>将修改好的代码编译打包，执行后获得一下结果：</p>
<pre><code>[root@5b49e23aa62e ufo2]# hdfs dfs -cat output_ufo2/part-00000
Alabama 548
Alaska  234
Arizona 2097
Arkansas        534
California      7679
Colorado        1457
Connecticut     608
Delaware        127
...
</code></pre>
<h2><a id="4%E3%80%81%E5%AE%9E%E7%8E%B0%E8%AE%A1%E6%95%B0%E5%99%A8%EF%BC%8C%E7%BB%9F%E8%AE%A1%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E6%95%B0%E9%87%8F" class="anchor" aria-hidden="true" href="#4%E3%80%81%E5%AE%9E%E7%8E%B0%E8%AE%A1%E6%95%B0%E5%99%A8%EF%BC%8C%E7%BB%9F%E8%AE%A1%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E6%95%B0%E9%87%8F"><span class="octicon octicon-link"></span></a>4、实现计数器，统计被忽略的数据数量</h2>
<p>上面的代码，统计了每个州的数据汇总信息，但是我们在汇总的时候，过滤掉了部分不满足统计条件的数据，这部分数据量不知道是多少，下面我们使用自定义的计数器，来统计一下这部分数据数量。
修改<code>UFORecordValidationMapper.java</code>文件，改造如下：</p>
<pre><code class="language-java">import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;

import java.io.IOException;

public class UFORecordValidationMapper extends MapReduceBase implements Mapper&lt;LongWritable, Text, LongWritable, Text&gt; {
    public enum LineCounters {
        BAD_LINES,
        TOO_MANY_TABS,
        TOO_FEW_TABS
    }

    @Override
    public void map(LongWritable key, Text value, OutputCollector&lt;LongWritable, Text&gt; output, Reporter reporter) throws IOException {
        String line = value.toString();
        if (validate(line, reporter))
            output.collect(key, value);
    }

    private boolean validate(String str, Reporter reporter) {
        String[] parts = str.split(&quot;\t&quot;);
        if (parts.length != 6) {
            if (parts.length &lt; 6) {
                reporter.incrCounter(LineCounters.TOO_FEW_TABS, 1);
            } else {
                reporter.incrCounter(LineCounters.TOO_MANY_TABS, 1);
            }
            reporter.incrCounter(LineCounters.BAD_LINES, 1);
            if ((reporter.getCounter(LineCounters.BAD_LINES).getCounter() % 10) == 0) {
                reporter.setStatus(&quot;Got 10 bad lines.&quot;);
                System.err.println(&quot;Read another 10 bad lines.&quot;);
            }
            return false;
        }

        return true;
    }
}

</code></pre>
<p>编译运行：</p>
<pre><code class="language-shell">&gt; hadoop jar ./ufo2.jar UFOLocation /user/root/ufo.tsv output_ufo3
...
UFORecordValidationMapper$LineCounters
                BAD_LINES=326
                TOO_FEW_TABS=2
                TOO_MANY_TABS=324
...
</code></pre>
<blockquote>
<p>假如你用的是Hadoop context对象API，就要用Context.getCounter(). increment()方法访问计数器。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[4-小白学习大数据之脚本语言提交Hadoop任务]]></title>
    <link href="jythons.github.io/16926846962945.html"/>
    <updated>2023-08-22T14:11:36+08:00</updated>
    <id>jythons.github.io/16926846962945.html</id>
    <content type="html"><![CDATA[
<p>除了使用Java去开发Hadoop任务处理逻辑，还可以使用其他语言进行开发，例如: Python，这一次我们使用Python编写Mapper和Reduce，来处理UFO目击事件的数据。数据源可以到网上下载。</p>
<span id="more"></span><!-- more -->
<h2><a id="1%E3%80%81%E7%8E%AF%E5%A2%83%E7%89%88%E6%9C%AC" class="anchor" aria-hidden="true" href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E7%89%88%E6%9C%AC"><span class="octicon octicon-link"></span></a>1、环境版本</h2>
<ul>
<li>Hadoop Version 3.3.6</li>
<li>Python Version 3.6.8</li>
</ul>
<h2><a id="2%E3%80%81%E4%BB%A3%E7%A0%81" class="anchor" aria-hidden="true" href="#2%E3%80%81%E4%BB%A3%E7%A0%81"><span class="octicon octicon-link"></span></a>2、代码</h2>
<p>本次测试代码主要统计UFO的形状。</p>
<h3><a id="2-1%E3%80%81mapper" class="anchor" aria-hidden="true" href="#2-1%E3%80%81mapper"><span class="octicon octicon-link"></span></a>2.1、Mapper</h3>
<pre><code class="language-python">#! /usr/bin/env python3
# -*- coding:UTF-8 -*-

import sys

# sys.stdin为读取数据，遍历读入数据的每一行
for line in sys.stdin:     
    #删除开头和结尾的空格
    line = line.strip()   
    # 以默认空格分隔行单词到words列表
    words = line.split('\t')  
    if len(words) == 6 and words[3]:
        print('%s\t%s' %(words[3],1))
	
</code></pre>
<blockquote>
<p>代码接收文件的每一行数据，使用制表符进行分割，过滤掉数据不全和没有形状为空的垃圾数据（有时候垃圾数据也需要使用，看具体情况）</p>
</blockquote>
<h3><a id="2-2%E3%80%81reduce" class="anchor" aria-hidden="true" href="#2-2%E3%80%81reduce"><span class="octicon octicon-link"></span></a>2.2、Reduce</h3>
<pre><code class="language-python">#!/usr/bin/env python3
# -*- coding:UTF-8 -*-

#from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# line = &quot;a	1&quot;
for line in sys.stdin:
	line = line.strip()

	word,count = line.split('\t', 1)
	# print(count)
	try:
	    count = int(count)
	except ValueError:
		# print(1)
	    continue

	if current_word == word:
	    current_count += count
	else:
	    if current_word:
	        print ('%s\t%s' % (current_word, current_count))
	    current_count = count
	    current_word = word
	        
if current_word == word:
    print ('%s\t%s' % (current_word, current_count))

</code></pre>
<h2><a id="3%E3%80%81%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1" class="anchor" aria-hidden="true" href="#3%E3%80%81%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="octicon octicon-link"></span></a>3、提交任务</h2>
<p>Mapper和Reduce代码开发好了之后，就可以调用Hadoop命令提交任务了</p>
<pre><code class="language-shell">hadoop jar /usr/local/hadoop/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -mapper &quot;python3 /Users/local/code/python/mapper.py&quot; -reducer &quot;python3 /Users/local/code/python/reducer.py&quot; -input /user/root/ufo.tsv -output /output_dir -file /Users/local/code/python/mapper.py -file /Users/local/code/python/reducer.py
</code></pre>
<blockquote>
<p>hadoop-streaming-3.3.6.jar：这个是使用流的方式提交任务的jar包<br />
-mapper：指向的是刚才编写的Mapper文件<br />
-reducer：指向的是刚才编写的Reduce文件<br />
-input：指向的是待分析的UFO数据文件（放到hdfs内）<br />
-file：两个file分别指向刚才写的Mapper和Reduce文件<br />
-output：输出文件夹（hdfs内的文件夹）</p>
</blockquote>
<h2><a id="4%E3%80%81%E7%AE%80%E5%8D%95%E7%BB%9F%E8%AE%A1" class="anchor" aria-hidden="true" href="#4%E3%80%81%E7%AE%80%E5%8D%95%E7%BB%9F%E8%AE%A1"><span class="octicon octicon-link"></span></a>4、简单统计</h2>
<p>在开发过程中，为了测试代码或数据规模比较小的情况下，也可以使用如下命令进行统计测试。</p>
<pre><code class="language-shell">cat ufo.tsv | mapper.py | sort | reduce.py
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[3-小白学习大数据之提交第一个任务]]></title>
    <link href="jythons.github.io/16926006824822.html"/>
    <updated>2023-08-21T14:51:22+08:00</updated>
    <id>jythons.github.io/16926006824822.html</id>
    <content type="html"><![CDATA[
<p>编写第一个WordCountJob程序，用来统计文件内每个单词的数量，通过这个小例子，了解一下Hadoop程序开发的过程。</p>
<span id="more"></span><!-- more -->
<h2><a id="1%E3%80%81%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F" class="anchor" aria-hidden="true" href="#1%E3%80%81%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="octicon octicon-link"></span></a>1、配置环境变量</h2>
<p>Hadoop版本使用的是3.3.6，其他低版本可自行百度，因为引入的jar包不同。Hadoop3.x版本使用如下Jar包：</p>
<pre><code class="language-java">hadoop-client-api-3.3.6.jar
hadoop-common-3.3.6.jar
hadoop-hdfs-3.3.6.jar
hadoop-mapreduce-client-core-3.3.6.jar
</code></pre>
<p>在环境变量配置文件<code>.bash_profile</code>里面设置classpath，配置完：<code>source .bash_profile</code></p>
<blockquote>
<p>JAVA_HOME和HADOOP_HOME的路径根据自己实际情况自行修改。</p>
</blockquote>
<pre><code class="language-shell">export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH
export HADOOP_HOME=/usr/local/hadoop/hadoop-3.3.6
export CLASSPATH=.:${HADOOP_HOME}/share/hadoop/common/hadoop-common-3.3.6.jar:${CLASSPATH}
export CLASSPATH=.:${HADOOP_HOME}/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:${CLASSPATH}
export CLASSPATH=.:${HADOOP_HOME}/share/hadoop/client/hadoop-client-api-3.3.6.jar:${CLASSPATH}
export CLASSPATH=.:${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:${CLASSPATH}
</code></pre>
<h2><a id="2%E3%80%81%E7%BC%96%E5%86%99%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F-wordcountjob-java" class="anchor" aria-hidden="true" href="#2%E3%80%81%E7%BC%96%E5%86%99%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F-wordcountjob-java"><span class="octicon octicon-link"></span></a>2、编写第一个程序 WordCountJob.java</h2>
<p>程序通过传入一个文本文件，统计文本中每个词语出现的数量。</p>
<pre><code class="language-java">import org.apache.hadoop.conf.Configuration;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.util.StringTokenizer;

import javax.naming.Context;
@SuppressWarnings(&quot;unused&quot;)
//自定义mapper类
public class WordCountJob{
    //map要接受的输入是&lt;k1,v1&gt;键值对，k1是当前行的偏移量，v1是内容；
    //输出是k2,v2;k2为单词，v2是数字
    public static class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;{
        protected void map(LongWritable k1,Text v1,Context context)throws IOException,InterruptedException{
            //对获取到的每一行数据进行切割，将单词切割出来
            String words[]=v1.toString().split(&quot; &quot;);
            for(String word:words){
                //把迭代出来的单词封装成&lt;k2,v2&gt;形式
                Text k2=new Text(word);
                LongWritable v2=new LongWritable(1l);
                //把&lt;k2,v2&gt;写出去
                context.write(k2,v2);
            }
        }
    }

    //reduce接受的输入是map的输出，输出k3,v3,k3是单词，v3是数字
    public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;{
        //针对k2s的数据进行累加求和，并且转换为k3，v3输出
        public void reduce(Text k2,Iterable&lt;LongWritable&gt; v2s,Context context)
                throws IOException,InterruptedException{
            //创建一个sum变量，保存v2s的和
            long sum=0l;
            for(LongWritable v2:v2s){
                sum+=v2.get();//保持类型一致
            }
            //组装k3，v3
            Text k3=k2;
            LongWritable v3=new LongWritable(sum);
            context.write(k3,v3);
        }
    }

    //组装job=map+reduce
    public static void main(String []args) {
        try{
            if(args.length!=2){
                System.exit(100);
            }
            //job需要的配置参数
            Configuration conf = new  Configuration();

            //创建一个job
            Job job = Job.getInstance(conf);


            //这一行必须设置，否则在集群中执行时找不到这个类
            job.setJarByClass(WordCountJob.class);
            //指定输入路径
            FileInputFormat.setInputPaths(job, new Path(args[0]));
            //指定输出目录
            FileOutputFormat.setOutputPath(job, new Path(args[1]));

            //指定map相关的代码
            job.setMapperClass(MyMapper.class);
            //指定k2的类型
            job.setMapOutputKeyClass(Text.class);
            //指定v2类型
            job.setMapOutputValueClass(LongWritable.class);


            //指定reduce相关的代码
            job.setReducerClass(MyReducer.class);
            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(LongWritable.class);

            job.waitForCompletion(true);

        }catch(Exception e){
            e.printStackTrace();
        }
    }

}
</code></pre>
<h3><a id="2-1%E3%80%81%E7%BC%96%E8%AF%91java%E6%96%87%E4%BB%B6" class="anchor" aria-hidden="true" href="#2-1%E3%80%81%E7%BC%96%E8%AF%91java%E6%96%87%E4%BB%B6"><span class="octicon octicon-link"></span></a>2.1、编译Java文件</h3>
<pre><code class="language-shell"># 编译.java文件
javac WordCountJob.java
</code></pre>
<h3><a id="2-2%E6%89%93%E5%8C%85-class%E6%96%87%E4%BB%B6%E4%B8%BAjar%E5%8C%85" class="anchor" aria-hidden="true" href="#2-2%E6%89%93%E5%8C%85-class%E6%96%87%E4%BB%B6%E4%B8%BAjar%E5%8C%85"><span class="octicon octicon-link"></span></a>2.2 打包.class文件为jar包</h3>
<pre><code class="language-shell">jar cvf wc1.jar WordCountJob*class
</code></pre>
<blockquote>
<p>wc1.jar：这个是打完包后的jar包名称<br />
WordCountJob*class：这个是编译后的所有.class文件</p>
</blockquote>
<h2><a id="3%E3%80%81%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1" class="anchor" aria-hidden="true" href="#3%E3%80%81%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="octicon octicon-link"></span></a>3、提交任务</h2>
<p>通过上述操作之后，会生成一个wc1.jar的jar包。在提交任务之前，我们创建一个测试文件：test.txt， 向文件内写入以下信息：</p>
<pre><code>This is a map
This is a reduce
This is a wordcount example
</code></pre>
<p>因为文件在本地磁盘内，需要将文件复制到hdfs内才可以使用。复制命令如下：</p>
<pre><code class="language-shell">hdfs dfs -copyFromLocal ./test.txt /user/root
</code></pre>
<blockquote>
<p><code>/user/root</code>这个目录不存在需要自行创建：<code>hdfs dfs -mkdir /user/root</code></p>
</blockquote>
<p>最后一步就是提交任务啦！</p>
<pre><code class="language-shell">hadoop jar wc1.jar WordCountJob test.txt output
</code></pre>
<blockquote>
<p>我们首次对自己的代码使用Hadoop JAR命令。它有4个参数:<br />
(1) JAR文件名;<br />
(2) JAR文件中的驱动类名;<br />
(3) 输入文件在HDFS的位置(本例中，是对/user/Hadoop home文件夹的相对引用);<br />
(4) 输出文件夹的目标位置(同样也是一个相对路径)。</p>
</blockquote>
<h2><a id="4%E3%80%81%E6%9F%A5%E7%9C%8B%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C" class="anchor" aria-hidden="true" href="#4%E3%80%81%E6%9F%A5%E7%9C%8B%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="octicon octicon-link"></span></a>4、查看任务执行结果</h2>
<p>使用如下命令查看任务结果文件：</p>
<pre><code class="language-shell">[root@5b49e23aa62e code]# hdfs dfs -cat /user/root/output/part-r-00000
This    3
a       3
example 1
is      3
map     1
reduce  1
wordcount       1
</code></pre>
<blockquote>
<p>/user/root/output/part-r-00000: 因为提交任务的时候使用的是output，所以他会以/user/root为根目录创建一个output文件夹，将结果输出到这里，如果要重复执行，输出目录不变的话，需要先将这个文件夹删除（<code>hadoop fs -rmr output</code>），这是Hadoop对与输出结构的保护机制。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[2-小白学习大数据之HDFS常用命令]]></title>
    <link href="jythons.github.io/16925850269059.html"/>
    <updated>2023-08-21T10:30:26+08:00</updated>
    <id>jythons.github.io/16925850269059.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>因为我安装的Hadoop版本是3.3.6，与老的版本有些不同。</p>
</blockquote>
<span id="more"></span><!-- more -->
<pre><code class="language-shell"># 创建文件夹
hdfs dfs -mkdir /user
# 查看文件夹内容
hdfs dfs -ls /
# 复制文件
hdfs dfs -cp test.txt data
# 拷贝本地文件到hdfs
hdfs dfs -copyFromLocal ./test.txt
# 拷贝hdfs文件到本地
hdfs dfs -copyToLocal ./test.txt  
# 删除文件
hdfs dfs -rm test.txt
</code></pre>
<p>持续补充...</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[1-小白学习大数据之本地环境搭建]]></title>
    <link href="jythons.github.io/16925808147985.html"/>
    <updated>2023-08-21T09:20:14+08:00</updated>
    <id>jythons.github.io/16925808147985.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>记录一下学习笔记，文章写的不好，如果有建议或意见请指教。</p>
</blockquote>
<p>本地环境我使用的是docker的centos7的镜像搭建的（这里推荐使用centos8）, 也可以使用虚拟机，安装方式大同小异。</p>
<ul>
<li>环境版本：
<ul>
<li>Java Version 8</li>
<li>Hadoop Version 3.3.6</li>
</ul>
</li>
</ul>
<span id="more"></span><!-- more -->
<h2><a id="1%E3%80%81centos7%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85" class="anchor" aria-hidden="true" href="#1%E3%80%81centos7%E5%AE%B9%E5%99%A8%E5%AE%89%E8%A3%85"><span class="octicon octicon-link"></span></a>1、CentOS7容器安装</h2>
<p>安装CentOS7容器的方式可以参考这篇文章：<a href="16923232842455.html">Docker安装练手centos7</a><br />
CentOS7容器配置后以后，使用如下命令启动一个新的容器：</p>
<pre><code class="language-shell">docker run -itd --name Centos7-my -p 18088:8088 -p 19870:9870 -p 10022:22 -p 10080:80 --privileged=true -v /Users/jiaoyang/Documents/works/centos7/user/root:/Users/local/  jythons-centos7-hadoop /usr/sbin/init
</code></pre>
<blockquote>
<p>--name：容器名字（可以自定义）<br />
-p：与宿主机绑定的端口号，左面是宿主机的端口，右面是容器的端口<br />
--privileged：这个参数等于true，标识容器可以完全获取到root权限，这里需要修改docker配置文件：<code>~/Library/Group\ Containers/group.com.docker/settings.json</code>的<code>deprecatedCgroupv1 = true</code>参数，才能生效。<br />
-v：用于映射宿主机目录和容器文件夹，用于同步文件，左面是宿主机，右面是容器</p>
</blockquote>
<p>这里注意，安装好容器后，记得关闭centos的防火墙。</p>
<h3><a id="1-1%E3%80%81docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4" class="anchor" aria-hidden="true" href="#1-1%E3%80%81docker%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="octicon octicon-link"></span></a>1.1、Docker基本使用命令</h3>
<pre><code class="language-shell">docker ps # 查看正在运行的容器，主要是查看容器ID
docker start [容器ID] # 启动容器
docker restart [容器ID] # 重启容器
docker stop [容器ID] # 停止容器

# 提交容器，如果需要新增映射端口、文件夹，时，需要先提交容器保存为镜像，然后基于该镜像重新启动一个新的容器，之前配置的环境都不会改变。 
docker commit [容器ID] [容器名] 
</code></pre>
<h2><a id="2%E3%80%81java%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85" class="anchor" aria-hidden="true" href="#2%E3%80%81java%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="octicon octicon-link"></span></a>2、Java环境安装</h2>
<p>Hadoop运行依赖Java环境，因此首先安装Java环境，我使用的是<a href="https://download.oracle.com/otn/java/jdk/8u381-b09/8c876547113c4e4aab3c868e9e0ec572/jdk-8u381-linux-aarch64.tar.gz">Java8</a>。<br />
下载好Java后，解压文件，将解压的文件夹放到<code>/usr/local/java</code>文件夹下，然后配置环境变量；</p>
<pre><code class="language-shell">export JAVA_HOME=/usr/local/java/jdk1.8.0_381
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH
export JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin
export PATH=$PATH:${JAVA_PATH}
</code></pre>
<h2><a id="3%E3%80%81hadoop%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85" class="anchor" aria-hidden="true" href="#3%E3%80%81hadoop%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="octicon octicon-link"></span></a>3、Hadoop环境安装</h2>
<p>下载<a href="https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz">Hadoop</a>安装包，解压到<code>/usr/local/hadoop</code>文件夹下，然后配置Hadoop环境变量：</p>
<pre><code class="language-shell">export HADOOP_HOME=/usr/local/hadoop/hadoop-3.3.6
export PATH=$PATH:${HADOOP_HOME}/bin
</code></pre>
<p>Hadoop有3种运行模式，各种模式下，Hadoop组件的运行场所有所不同。HDFS包括 一个NameNode，它充当着集群协调者的角色，是一个或多个用于存储数据的DataNode的管理者。 对于MapReduce而言，JobTracker是集群的主节点，它负责协调多个TaskTracker进程执行的工作。 Hadoop以如下3种模式部署上述组件。</p>
<ul>
<li>本地独立模式:像前面计算圆周率的例子一样，如果不进行任何配置的话，这是Hadoop 的默认工作模式。在这种模式下，Hadoop的所有组件，如NameNode、DataNode、JobTracker 和TaskTracker，都运行在同一个Java进程中。</li>
<li>伪分布式模式:在这种模式下，Hadoop的各个组件都拥有一个单独的Java虚拟机，它们 之间通过网络套接字通信。这种模式在一台主机上有效地产生了一个具有完整功能的微 型集群。</li>
<li>完全分布式模式:在这种模式下，Hadoop分布在多台主机上，其中一些是通用的工作机， 其余的是组件的专用主机，比如NameNode和JobTracker。</li>
</ul>
<p>每种模式都有其优点和缺点。完全分布式模式显然是唯一一种可以将Hadoop扩展到机器集 群的方式，但它需要更多的配置工作，更不用提所需要的机器集群。本地或独立模式的设置工作 是最简单的，但它与用户的交互方式不同于全分布式模式的交互方式。伪分布式模式，程序在一台主机上运行，但是在伪分布式模式下执行的操作与其在更大的集群上的运作几乎是相同的。因此我们也使用伪分布式模式来学习。</p>
<h3><a id="3-1%E3%80%81%E9%85%8D%E7%BD%AEhadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%A8%A1%E5%BC%8F" class="anchor" aria-hidden="true" href="#3-1%E3%80%81%E9%85%8D%E7%BD%AEhadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%A8%A1%E5%BC%8F"><span class="octicon octicon-link"></span></a>3.1、配置Hadoop伪分布式模式</h3>
<p>进入到Hadoop配置文件文件夹：</p>
<pre><code class="language-shell">cd /usr/local/hadoop/hadoop-3.3.6/etc/hadoop
</code></pre>
<blockquote>
<p>这里低版本的Hadoop配置文件文件夹与新版本不同，需要注意下。</p>
</blockquote>
<p>查看Hadoop安装包中的conf目录。那里有很多配置文件，但只需对其中3个文件进行修改:core-site.xml、hdfs-site.xml和mapred-site.xml</p>
<p><strong>修改core-site.xml文件:</strong></p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://[容器ID]:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p><strong>修改hdfs-site.xml文件:</strong></p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p><strong>修改mapred-site.xml文件:</strong></p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;
        &lt;value&gt;127.0.0.1:9001&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<h3><a id="3-2%E3%80%81%E4%BF%AE%E6%94%B9-hdfs%E7%9A%84%E6%A0%B9%E7%9B%AE%E5%BD%95" class="anchor" aria-hidden="true" href="#3-2%E3%80%81%E4%BF%AE%E6%94%B9-hdfs%E7%9A%84%E6%A0%B9%E7%9B%AE%E5%BD%95"><span class="octicon octicon-link"></span></a>3.2、修改 HDFS 的根目录</h3>
<p>由于我们将在Hadoop中存储数据，同时所有组件都运行在本地主机，这些数据都需要存储 在本地文件系统的某个地方。不管选择了何种模式，Hadoop默认使用hadoop.tmp.dir属性作为 根目录，所有文件和数据都将写入该目录。危险的是， hadoop.tmp.dir的默认值为/tmp，一些版本的Linux系统会在每次重新启动时删除/tmp的内容。所以，明确规定数据留存的位置更为安全。
执行如下步骤，修改HDFS 的根目录：</p>
<p><strong>1.创建Hadoop保存数据的目录</strong></p>
<pre><code class="language-shell">mkdir /var/lib/hadoop
</code></pre>
<p><strong>2.确保任何用户都可在此目录写入数据</strong></p>
<pre><code class="language-shell">chmod 777 /var/lib/hadoop
</code></pre>
<p><strong>3.再次修改core-site.xml文件，添加下列属性</strong></p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://[容器ID|主机名]:9000&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/var/lib/hadoop&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

</code></pre>
<h3><a id="3-3%E3%80%81%E4%BF%AE%E6%94%B9%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90" class="anchor" aria-hidden="true" href="#3-3%E3%80%81%E4%BF%AE%E6%94%B9%E5%90%AF%E5%8A%A8%E6%96%87%E4%BB%B6%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90"><span class="octicon octicon-link"></span></a>3.3、修改启动文件用户权限</h3>
<p>如果不修改启动文件用户权限，则会报如下错误：</p>
<pre><code class="language-shell">Starting namenodes on [namenode]
ERROR: Attempting to operate on hdfs namenode as root
ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.
Starting datanodes
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
Starting secondary namenodes [datanode1]
ERROR: Attempting to operate on hdfs secondarynamenode as root
ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.
Starting resourcemanager
ERROR: Attempting to operate on yarn resourcemanager as root
ERROR: but there is no YARN_RESOURCEMANAGER_USER defined. Aborting operation.
Starting nodemanagers
ERROR: Attempting to operate on yarn nodemanager as root
ERROR: but there is no YARN_NODEMANAGER_USER defined. Aborting operation.

</code></pre>
<p>解决这个错误，只需要在如下文件内的开头位置，添加这些配置即可。<br />
<strong>hadoop/sbin/start-dfs.sh 和 stop-dfs.sh文件</strong></p>
<pre><code class="language-shell">HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root 
</code></pre>
<p><strong>hadoop/sbin/start-yarn.sh 和 stop-yarn.sh文件</strong></p>
<pre><code class="language-shell">YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
</code></pre>
<p>每次修改配置以后，都需要执行以下命令，使修改的配置生效：</p>
<pre><code class="language-shell">hdfs namenode -format
</code></pre>
<h3><a id="3-4%E3%80%81%E7%94%9F%E6%88%90ssh%E7%A7%98%E9%92%A5%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%9C%8D%E5%8A%A1%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95" class="anchor" aria-hidden="true" href="#3-4%E3%80%81%E7%94%9F%E6%88%90ssh%E7%A7%98%E9%92%A5%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%9C%8D%E5%8A%A1%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95"><span class="octicon octicon-link"></span></a>3.4、生成ssh秘钥，用于服务远程登录</h3>
<pre><code class="language-shell">&gt; ssh-keygen -t rsa -P &quot;&quot; # 生成ssh秘钥
&gt; ssh-copy-id  # 将ssh秘钥copy到authorized_keys，也可以（执行命令：cat id_rsa.pub &gt;&gt; authorized_keys）
</code></pre>
<blockquote>
<p>如果不生成ssh秘钥，启动Hadoop服务时，会报如下错误：Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password)</p>
</blockquote>
<h2><a id="4%E3%80%81%E5%90%AF%E5%8A%A8hadoop%E6%9C%8D%E5%8A%A1" class="anchor" aria-hidden="true" href="#4%E3%80%81%E5%90%AF%E5%8A%A8hadoop%E6%9C%8D%E5%8A%A1"><span class="octicon octicon-link"></span></a>4、启动Hadoop服务</h2>
<p>启动服务之前，还需要设置JavaHome路径，打开<code>/usr/local/hadoop/hadoop-3.3.6/etc/hadoop</code>路径下的配置文件：<code>Hadoop-env.sh</code>，找到export JAVA_HOME=选项，将之前的JAVA_HOME路径写到这里，打开注释。<br />
通过上面的配置后，下面就可以启动Hadoop服务了，使用如下命令：</p>
<pre><code class="language-shell">## 启动服务
/usr/local/hadoop/hadoop-3.3.6/sbin/start-all.sh
## 停止服务
/usr/local/hadoop/hadoop-3.3.6/sbin/stop-all.sh
</code></pre>
<p>使用jps命令，查看启动起来的进程：</p>
<pre><code class="language-shell">[root@88fec458a0c4 hadoop]# jps
16133 Jps
8664 DataNode
8872 SecondaryNameNode
9448 NodeManager
8495 NameNode
9119 ResourceManager
</code></pre>
<h2><a id="5%E3%80%81%E8%A1%A5%E5%85%85" class="anchor" aria-hidden="true" href="#5%E3%80%81%E8%A1%A5%E5%85%85"><span class="octicon octicon-link"></span></a>5、补充</h2>
<p>Hadoop3.x版本与老的版本端口号发生了改变下面是对应关系：</p>
<table>
<thead>
<tr>
<th>服务</th>
<th>老端口</th>
<th>新端口</th>
</tr>
</thead>
<tbody>
<tr>
<td>NameNode</td>
<td>50470</td>
<td>9871</td>
</tr>
<tr>
<td></td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td></td>
<td>8020</td>
<td>9820</td>
</tr>
<tr>
<td>Secondary NN</td>
<td>50091</td>
<td>9869</td>
</tr>
<tr>
<td></td>
<td>50090</td>
<td>9868</td>
</tr>
<tr>
<td>Datanode</td>
<td>50020</td>
<td>9867</td>
</tr>
<tr>
<td></td>
<td>50010</td>
<td>9866</td>
</tr>
<tr>
<td></td>
<td>50475</td>
<td>9865</td>
</tr>
<tr>
<td></td>
<td>50075</td>
<td>9864</td>
</tr>
</tbody>
</table>
<h2><a id="6%E3%80%81%E6%89%A7%E8%A1%8Chdfs-namenode-format%E5%90%8E-datanode%E6%B2%A1%E6%9C%89%E5%90%AF%E5%8A%A8" class="anchor" aria-hidden="true" href="#6%E3%80%81%E6%89%A7%E8%A1%8Chdfs-namenode-format%E5%90%8E-datanode%E6%B2%A1%E6%9C%89%E5%90%AF%E5%8A%A8"><span class="octicon octicon-link"></span></a>6、执行hdfs namenode -format后 datanode没有启动</h2>
<p>当执行<code>hdfs namenode -format</code>后，没有启动datanode服务，原因是因为格式化之后namenode的clusterID变更，和datanode的clusterID不一致，导致。<br />
<strong>解决办法：</strong><br />
进入到dfs目录<code>cd /var/lib/hadoop/dfs/</code><br />
复制<code>/var/lib/hadoop/dfs/name/current/VERSION</code>文件内的<code>clusterID</code>到<code>/var/lib/hadoop/dfs/data/current/VERSION</code>文件内的<code>clusterID</code>即可。
复制之后数据会丢失，数据备份方法后续补充...</p>
<blockquote>
<p><code>/var/lib/hadoop/</code>目录是配置文件core-site.xml内配置的。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker安装练手centos7]]></title>
    <link href="jythons.github.io/16923232842455.html"/>
    <updated>2023-08-18T09:48:04+08:00</updated>
    <id>jythons.github.io/16923232842455.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85docker" class="anchor" aria-hidden="true" href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85docker"><span class="octicon octicon-link"></span></a>一、安装Docker</h2>
<p>参见文章：<a href="https://blog.csdn.net/y1534414425/article/details/107872715">https://blog.csdn.net/y1534414425/article/details/107872715</a></p>
<h2><a id="%E4%BA%8C%E3%80%81%E6%8B%89%E5%8F%96centos7%E9%95%9C%E5%83%8F" class="anchor" aria-hidden="true" href="#%E4%BA%8C%E3%80%81%E6%8B%89%E5%8F%96centos7%E9%95%9C%E5%83%8F"><span class="octicon octicon-link"></span></a>二、拉取Centos7镜像</h2>
<pre><code class="language-shell">docker pull centos:7
</code></pre>
<h2><a id="%E4%B8%89%E3%80%81%E8%BF%90%E8%A1%8Ccentos7" class="anchor" aria-hidden="true" href="#%E4%B8%89%E3%80%81%E8%BF%90%E8%A1%8Ccentos7"><span class="octicon octicon-link"></span></a>三、运行Centos7</h2>
<pre><code class="language-shell">docker run -itd --name myCentos -p 8081:22 -p 8082:80 centos:7 /bin/bash
</code></pre>
<h2><a id="%E5%9B%9B%E3%80%81%E8%BF%9B%E5%85%A5%E5%AE%B9%E5%99%A8" class="anchor" aria-hidden="true" href="#%E5%9B%9B%E3%80%81%E8%BF%9B%E5%85%A5%E5%AE%B9%E5%99%A8"><span class="octicon octicon-link"></span></a>四、进入容器</h2>
<pre><code class="language-shell">docker attach [容器ID]
</code></pre>
<h2><a id="%E4%BA%94%E3%80%81%E6%A0%B9%E6%8D%AE%E8%87%AA%E5%B7%B1%E9%9C%80%E6%B1%82%EF%BC%8C%E5%85%88%E5%AE%89%E8%A3%85%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%9A%84%EF%BC%88%E5%AE%B9%E5%99%A8%EF%BC%8C%E9%BB%98%E8%AE%A4%E6%98%AFroot%E7%94%A8%E6%88%B7%EF%BC%89" class="anchor" aria-hidden="true" href="#%E4%BA%94%E3%80%81%E6%A0%B9%E6%8D%AE%E8%87%AA%E5%B7%B1%E9%9C%80%E6%B1%82%EF%BC%8C%E5%85%88%E5%AE%89%E8%A3%85%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E7%9A%84%EF%BC%88%E5%AE%B9%E5%99%A8%EF%BC%8C%E9%BB%98%E8%AE%A4%E6%98%AFroot%E7%94%A8%E6%88%B7%EF%BC%89"><span class="octicon octicon-link"></span></a>五、根据自己需求，先安装一些基本的（容器，默认是root用户）</h2>
<pre><code class="language-shell">yum install -y net-tools
</code></pre>
<h2><a id="%E5%85%AD%E3%80%81%E6%8E%A5%E7%9D%80%E5%AE%89%E8%A3%85openssl%EF%BC%8Copenssh-server" class="anchor" aria-hidden="true" href="#%E5%85%AD%E3%80%81%E6%8E%A5%E7%9D%80%E5%AE%89%E8%A3%85openssl%EF%BC%8Copenssh-server"><span class="octicon octicon-link"></span></a>六、接着安装openssl，openssh-server</h2>
<pre><code class="language-shell">yum install -y openssl openssh-server
</code></pre>
<h2><a id="%E4%B8%83%E3%80%81%E7%84%B6%E5%90%8E%E5%90%AF%E5%8A%A8ssh" class="anchor" aria-hidden="true" href="#%E4%B8%83%E3%80%81%E7%84%B6%E5%90%8E%E5%90%AF%E5%8A%A8ssh"><span class="octicon octicon-link"></span></a>七、然后启动ssh</h2>
<pre><code class="language-shell">/usr/sbin/sshd -D
</code></pre>
<p>这里会报错</p>
<p>需要进行下面的设置</p>
<pre><code class="language-shell">&gt; ssh-keygen -q -t rsa -b 2048 -f /etc/ssh/ssh_host_rsa_key -N ''  
&gt; ssh-keygen -q -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -N ''
&gt; ssh-keygen -t dsa -f /etc/ssh/ssh_host_ed25519_key -N ''
</code></pre>
<h2><a id="%E5%85%AB%E3%80%81%E6%8E%A5%E7%9D%80%E4%BF%AE%E6%94%B9sshd-config%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF" class="anchor" aria-hidden="true" href="#%E5%85%AB%E3%80%81%E6%8E%A5%E7%9D%80%E4%BF%AE%E6%94%B9sshd-config%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="octicon octicon-link"></span></a>八、接着修改sshd_config文件配置信息</h2>
<p>配置文件路径为 <code>/etc/ssh/sshd_config</code></p>
<ol>
<li>将 Port 22 前面的注释去掉（开启22号端口）</li>
<li>将 PermitRootLogin的值改为 yes （这里是设置是否允许root用户登录，可根据自己需求决定是否开启）</li>
<li>重新启动ssh</li>
</ol>
<pre><code class="language-shell">&gt; /usr/sbin/sshd -D &amp;
</code></pre>
<blockquote>
<p>注意，如果设置都没问题的话，命令结尾加个‘&amp;’，自动后台运行，启动成功会返回进程号</p>
</blockquote>
<h2><a id="%E4%B9%9D%E3%80%81%E7%BB%99root%E6%B7%BB%E5%8A%A0%E5%AF%86%E7%A0%81%E6%B7%BB%E5%8A%A0%E8%BF%87%E7%9A%84%E5%8F%AF%E8%B7%B3%E8%BF%87%E6%AD%A4%E6%AD%A5%E9%AA%A4" class="anchor" aria-hidden="true" href="#%E4%B9%9D%E3%80%81%E7%BB%99root%E6%B7%BB%E5%8A%A0%E5%AF%86%E7%A0%81%E6%B7%BB%E5%8A%A0%E8%BF%87%E7%9A%84%E5%8F%AF%E8%B7%B3%E8%BF%87%E6%AD%A4%E6%AD%A5%E9%AA%A4"><span class="octicon octicon-link"></span></a>九、给root添加密码,添加过的可跳过此步骤</h2>
<pre><code class="language-shell">&gt; yum install passwd
&gt; passwd
</code></pre>
<h2><a id="%E5%8D%81%E3%80%81%E5%85%88%E9%80%80%E5%87%BA%E5%B9%B6%E5%85%B3%E9%97%AD%E5%88%9A%E6%89%8D%E7%9A%84%E5%AE%B9%E5%99%A8%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8exit%E5%91%BD%E4%BB%A4%EF%BC%8C%E7%84%B6%E5%90%8E%E4%BF%9D%E5%AD%98%E9%95%9C%E5%83%8F" class="anchor" aria-hidden="true" href="#%E5%8D%81%E3%80%81%E5%85%88%E9%80%80%E5%87%BA%E5%B9%B6%E5%85%B3%E9%97%AD%E5%88%9A%E6%89%8D%E7%9A%84%E5%AE%B9%E5%99%A8%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8exit%E5%91%BD%E4%BB%A4%EF%BC%8C%E7%84%B6%E5%90%8E%E4%BF%9D%E5%AD%98%E9%95%9C%E5%83%8F"><span class="octicon octicon-link"></span></a>十、先退出并关闭刚才的容器，可以使用 exit 命令，然后保存镜像</h2>
<pre><code class="language-shell">docker commit [容器ID] [镜像名称]
</code></pre>
<h2><a id="%E5%8D%81%E4%B8%80%E3%80%81%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%95%9C%E5%83%8F%E9%87%8D%E6%96%B0%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8" class="anchor" aria-hidden="true" href="#%E5%8D%81%E4%B8%80%E3%80%81%E5%9F%BA%E4%BA%8E%E6%96%B0%E9%95%9C%E5%83%8F%E9%87%8D%E6%96%B0%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8"><span class="octicon octicon-link"></span></a>十一、基于新镜像重新启动一个容器</h2>
<pre><code class="language-shell">docker run -itd --name newCentos -p 8081:22 -p 8082:80 [新镜像名称] /bin/bash
</code></pre>
<blockquote>
<p>注意：进入容器并重新开启ssh，不会自动启动
/usr/sbin/sshd -D &amp;</p>
</blockquote>
<h2><a id="%E5%8F%82%E8%80%83" class="anchor" aria-hidden="true" href="#%E5%8F%82%E8%80%83"><span class="octicon octicon-link"></span></a>参考</h2>
<p><a href="https://blog.csdn.net/y1534414425/article/details/108030323">https://blog.csdn.net/y1534414425/article/details/108030323</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac 执行node、php命令报错：/usr/local/opt/icu4c/lib/libicui18n.70.dylib‘ (no such file)]]></title>
    <link href="jythons.github.io/16922348850199.html"/>
    <updated>2023-08-17T09:14:45+08:00</updated>
    <id>jythons.github.io/16922348850199.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="1%E8%AF%A6%E7%BB%86%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF" class="anchor" aria-hidden="true" href="#1%E8%AF%A6%E7%BB%86%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF"><span class="octicon octicon-link"></span></a>1.详细错误信息</h2>
<pre><code class="language-shell">/usr/local/opt/php@7.2/bin/php -v                
dyld[49280]: Library not loaded: '/usr/local/opt/icu4c/lib/libicui18n.70.dylib'
  Referenced from: '/usr/local/Cellar/php@7.2/7.2.34_4/bin/php'
  Reason: tried: '/usr/local/opt/icu4c/lib/libicui18n.70.dylib' (no such file), '/usr/local/lib/libicui18n.70.dylib' (no such file), '/usr/lib/libicui18n.70.dylib' (no such file), '/usr/local/Cellar/icu4c/72.1/lib/libicui18n.70.dylib' (no such file), '/usr/local/lib/libicui18n.70.dylib' (no such file), '/usr/lib/libicui18n.70.dylib' (no such file)
[1]    49280 abort      /usr/local/opt/php@7.2/bin/php -v

</code></pre>
<h2><a id="2%E9%94%99%E8%AF%AF%E5%8E%9F%E5%9B%A0" class="anchor" aria-hidden="true" href="#2%E9%94%99%E8%AF%AF%E5%8E%9F%E5%9B%A0"><span class="octicon octicon-link"></span></a>2.错误原因</h2>
<p>出现这个错误是因为brew更新，导致库文件也被更新，之前安装的语言环境还是指向老的库版本文件，因此报错。但是老的库文件有的还在，可以找到老的库文件放到对应的目录下即可。</p>
<h2><a id="3%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95" class="anchor" aria-hidden="true" href="#3%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="octicon octicon-link"></span></a>3.解决办法</h2>
<ol>
<li>找到老的库文件放到对应的目录下即可。</li>
<li>卸载之前的语言，使用最新版本（推荐）
<pre><code class="language-shell">&gt; brew uninstall php@7.4
# 这里卸载老的语言版本后，可以去手动删除一下老的配置文件
&gt; cd /opt/homebrew/etc/php/
&gt; rm -rf {版本号}
&gt; brew install php
</code></pre>
</li>
</ol>
<h2><a id="4%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E6%96%87%E4%BB%B6" class="anchor" aria-hidden="true" href="#4%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E6%96%87%E4%BB%B6"><span class="octicon octicon-link"></span></a>4.可能遇到的文件</h2>
<p>我在使用方法1解决问题的时候，不小心将lib下的.dylib文件删除了，最后的解决办法是重新安装icu4c，就可以恢复之前库的文件。</p>
<h3><a id="4-1%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%E5%A6%82%E4%B8%8B" class="anchor" aria-hidden="true" href="#4-1%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%E5%A6%82%E4%B8%8B"><span class="octicon octicon-link"></span></a>4.1 安装方式如下</h3>
<pre><code class="language-shell">&gt; brew reinstall icu4c
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[php7.1以上版本安装ds拓展]]></title>
    <link href="jythons.github.io/16915611066736.html"/>
    <updated>2023-08-09T14:05:06+08:00</updated>
    <id>jythons.github.io/16915611066736.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E4%BD%BF%E7%94%A8pecl%E5%AE%89%E8%A3%85" class="anchor" aria-hidden="true" href="#%E4%BD%BF%E7%94%A8pecl%E5%AE%89%E8%A3%85"><span class="octicon octicon-link"></span></a>使用pecl安装</h2>
<h3><a id="%E6%8A%A5%E9%94%99" class="anchor" aria-hidden="true" href="#%E6%8A%A5%E9%94%99"><span class="octicon octicon-link"></span></a>报错</h3>
<pre><code class="language-shell">/opt/homebrew/Cellar/php@7.4/7.4.26_1/include/php/ext/pcre/php_pcre.h:25:10: fatal error: 'pcre2.h' file not found
</code></pre>
<h3><a id="%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95" class="anchor" aria-hidden="true" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="octicon octicon-link"></span></a>解决办法</h3>
<pre><code class="language-shell">ln -s /opt/homebrew/include/pcre2.h /opt/homebrew/Cellar/php@7.4/7.4.26_1/include/php/ext/pcre/pcre2.h
</code></pre>
<blockquote>
<p>在对应的报错目录下添加需要使用的文件即可</p>
</blockquote>
<h2><a id="ds%E6%8B%93%E5%B1%95%E4%BD%BF%E7%94%A8%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3" class="anchor" aria-hidden="true" href="#ds%E6%8B%93%E5%B1%95%E4%BD%BF%E7%94%A8%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3"><span class="octicon octicon-link"></span></a>DS拓展使用官方文档</h2>
<p><a href="https://www.php.net/manual/zh/class.ds-collection.php">https://www.php.net/manual/zh/class.ds-collection.php</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker日常命令（持续更新...）]]></title>
    <link href="jythons.github.io/16906398618868.html"/>
    <updated>2023-07-29T22:11:01+08:00</updated>
    <id>jythons.github.io/16906398618868.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E8%8E%B7%E5%8F%96%E5%AE%B9%E5%99%A8ip" class="anchor" aria-hidden="true" href="#%E8%8E%B7%E5%8F%96%E5%AE%B9%E5%99%A8ip"><span class="octicon octicon-link"></span></a>获取容器IP</h2>
<pre><code class="language-shell">docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' {容器ID}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[日常开发错误]]></title>
    <link href="jythons.github.io/16904527687916.html"/>
    <updated>2023-07-27T18:12:48+08:00</updated>
    <id>jythons.github.io/16904527687916.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E9%94%99%E8%AF%AF1" class="anchor" aria-hidden="true" href="#%E9%94%99%E8%AF%AF1"><span class="octicon octicon-link"></span></a>错误1</h2>
<p><strong>错误：</strong></p>
<blockquote>
<p>grpc客户端调用服务端时，报如下错误：</p>
</blockquote>
<pre><code class="language-go">grpc panic: runtime error: invalid memory address or nil pointer dereference
</code></pre>
<p><strong>原因：</strong><br />
可能使用的调用方法是老版本。<br />
<strong>解决：</strong><br />
使用如下方法连接服务端即可</p>
<pre><code class="language-go">conn, err := grpc.Dial(&quot;localhost:&quot;+port, grpc.WithTransportCredentials(insecure.NewCredentials()))
</code></pre>
<blockquote>
<p>友情提示：<br />
开发时，吧所有错误都判断一下，方便根据提示信息定位问题。不要偷懒，使用“_”忽略err。</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac下安装Protobuf]]></title>
    <link href="jythons.github.io/16901201569851.html"/>
    <updated>2023-07-23T21:49:16+08:00</updated>
    <id>jythons.github.io/16901201569851.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E5%AE%89%E8%A3%85protobuf" class="anchor" aria-hidden="true" href="#%E5%AE%89%E8%A3%85protobuf"><span class="octicon octicon-link"></span></a>安装protobuf</h2>
<pre><code class="language-shell">brew install protobuf
</code></pre>
<h2><a id="%E6%A3%80%E6%9F%A5%E5%AE%89%E8%A3%85%E7%BB%93%E6%9E%9C" class="anchor" aria-hidden="true" href="#%E6%A3%80%E6%9F%A5%E5%AE%89%E8%A3%85%E7%BB%93%E6%9E%9C"><span class="octicon octicon-link"></span></a>检查安装结果</h2>
<pre><code class="language-shell">protoc  --version
</code></pre>
<h2><a id="%E5%AE%89%E8%A3%85golang-for-protobuf%E6%8F%92%E4%BB%B6" class="anchor" aria-hidden="true" href="#%E5%AE%89%E8%A3%85golang-for-protobuf%E6%8F%92%E4%BB%B6"><span class="octicon octicon-link"></span></a>安装golang for protobuf插件</h2>
<pre><code class="language-shell">go get -u -v github.com/golang/protobuf/protoc-gen-go

go v1.17以上版本使用如下命令安装
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest
</code></pre>
<h2><a id="%E8%BF%90%E7%94%A8%EF%BC%8C%E5%9C%A8%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E9%87%8C%E8%BF%90%E8%A1%8C" class="anchor" aria-hidden="true" href="#%E8%BF%90%E7%94%A8%EF%BC%8C%E5%9C%A8%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E9%87%8C%E8%BF%90%E8%A1%8C"><span class="octicon octicon-link"></span></a>运用，在文件目录里运行</h2>
<pre><code class="language-shell"># protoc --go_out=plugins=grpc:. user.proto
protoc --go_out=. ./proto/*.proto

# 生成描述文件
protoc --go_out=./ ./proto/*.proto 
# 生成grpc描述文件
protoc --go-grpc_out=. ./proto/*.proto

# 也可以一起执行
protoc --go_out=. --go-grpc_out=. ./proto/*.proto
</code></pre>
<blockquote>
<p>开启go module 方式创建项目的时候，安装好的插件比如protoc-gen-go，可以使用go install 项目，的方式生成可执行文件到:$GROOT/bin下</p>
</blockquote>
<pre><code class="language-shell">go get google.golang.org/grpc
go get -u -v github.com/golang/protobuf/protoc-gen-go
go get -u google.golang.org/grpc/cmd/protoc-gen-go-grpc
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gorm1和Gorm2差异]]></title>
    <link href="jythons.github.io/16897602141334.html"/>
    <updated>2023-07-19T17:50:14+08:00</updated>
    <id>jythons.github.io/16897602141334.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<p><a href="https://blog.csdn.net/zzsan/article/details/120886965">https://blog.csdn.net/zzsan/article/details/120886965</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[面试宝典]]></title>
    <link href="jythons.github.io/16880312960537.html"/>
    <updated>2023-06-29T17:34:56+08:00</updated>
    <id>jythons.github.io/16880312960537.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="1%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80" class="anchor" aria-hidden="true" href="#1%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80"><span class="octicon octicon-link"></span></a>1. 语言基础</h2>
<h3><a id="1-1-make%E5%92%8Cnew" class="anchor" aria-hidden="true" href="#1-1-make%E5%92%8Cnew"><span class="octicon octicon-link"></span></a>1.1 make和new</h3>
<p>make只能创建引用类型，例如：slice、map、channel，并且返回引用类型；并且在分配完内存空间后会初始化
new可以创建值类型，会为其赋初始值，返回指针类型，new分配的空间会被清零</p>
<h3><a id="1-2%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E6%96%B9%E5%BC%8F" class="anchor" aria-hidden="true" href="#1-2%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E6%96%B9%E5%BC%8F"><span class="octicon octicon-link"></span></a>1.2 字符串拼接方式</h3>
<ul>
<li>使用“+”拼接，str = str1+str2;</li>
<li>使用 sprintf 拼接，str = sprintf(&quot;%s%d%s&quot;, str1, 1, str2)</li>
<li>使用join拼接，s = []string{str1,str2}; str = strings.Join(s, &quot;&quot;);</li>
</ul>
<h3><a id="1-3%E5%88%87%E7%89%87%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6" class="anchor" aria-hidden="true" href="#1-3%E5%88%87%E7%89%87%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6"><span class="octicon octicon-link"></span></a>1.3 切片扩容机制</h3>
<p>切片底层使用数组实现，与数组共享同一块内存空间，当使用append方法向切片内追加元素时，如果容量不足，会发生扩容，扩容时，会先申请一块内存空间，然后将数据copy过去，再将新的元素追加到新的内存空间。申请的内存空间机制是，如果原来的切片容量小于1024，则新的内存空间为原来的2倍，如果原来的切片容量大于等于1024，则内存空间为原来的1.25倍。</p>
<h3><a id="1-4%E9%94%81%E6%9C%89%E5%87%A0%E7%A7%8D%EF%BC%8C%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9" class="anchor" aria-hidden="true" href="#1-4%E9%94%81%E6%9C%89%E5%87%A0%E7%A7%8D%EF%BC%8C%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9"><span class="octicon octicon-link"></span></a>1.4 锁有几种，都有什么特点</h3>
<h3><a id="1-5-gorutine%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%EF%BC%88%E5%8D%8F%E6%88%90%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%EF%BC%89" class="anchor" aria-hidden="true" href="#1-5-gorutine%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%EF%BC%88%E5%8D%8F%E6%88%90%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%EF%BC%89"><span class="octicon octicon-link"></span></a>1.5 gorutine运行机制（协成的实现机制）</h3>
<h2><a id="2-gin%E6%A1%86%E6%9E%B6" class="anchor" aria-hidden="true" href="#2-gin%E6%A1%86%E6%9E%B6"><span class="octicon octicon-link"></span></a>2. gin框架</h2>
<h3><a id="2-1%E8%B7%AF%E7%94%B1%E5%AE%9E%E7%8E%B0" class="anchor" aria-hidden="true" href="#2-1%E8%B7%AF%E7%94%B1%E5%AE%9E%E7%8E%B0"><span class="octicon octicon-link"></span></a>2.1 路由实现</h3>
<h3><a id="2-2%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6" class="anchor" aria-hidden="true" href="#2-2%E5%AE%9A%E4%B9%89%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="octicon octicon-link"></span></a>2.2 定义中间件</h3>
<h3><a id="2-3%E5%89%8D%E7%BD%AE%E5%92%8C%E5%90%8E%E7%BD%AE%E4%B8%AD%E9%97%B4%E4%BB%B6" class="anchor" aria-hidden="true" href="#2-3%E5%89%8D%E7%BD%AE%E5%92%8C%E5%90%8E%E7%BD%AE%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="octicon octicon-link"></span></a>2.3 前置和后置中间件</h3>
<h2><a id="3-gorm" class="anchor" aria-hidden="true" href="#3-gorm"><span class="octicon octicon-link"></span></a>3. gorm</h2>
<h3><a id="3-1%E6%80%8E%E6%A0%B7%E4%BD%BF%E7%94%A8-gorm%E8%BF%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2" class="anchor" aria-hidden="true" href="#3-1%E6%80%8E%E6%A0%B7%E4%BD%BF%E7%94%A8-gorm%E8%BF%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2"><span class="octicon octicon-link"></span></a>3.1 怎样使用gorm连表查询</h3>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[webpack打包electron应用本地环境搭建]]></title>
    <link href="jythons.github.io/16870507293577.html"/>
    <updated>2023-06-18T09:12:09+08:00</updated>
    <id>jythons.github.io/16870507293577.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="webpack%E9%85%8D%E7%BD%AE" class="anchor" aria-hidden="true" href="#webpack%E9%85%8D%E7%BD%AE"><span class="octicon octicon-link"></span></a>webpack配置</h2>
<p>首先配置webpack打包target<br />
文档：<a href="https://webpack.docschina.org/configuration/target/">https://webpack.docschina.org/configuration/target/</a></p>
<pre><code>module.exports = {
  // electron 渲染应用
  target:'electron-renderer',
}
</code></pre>
<h2><a id="%E5%AE%89%E8%A3%85election%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83" class="anchor" aria-hidden="true" href="#%E5%AE%89%E8%A3%85election%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="octicon octicon-link"></span></a>安装election运行环境</h2>
<pre><code class="language-bash"># Clone this repository
git clone https://github.com/electron/electron-quick-start
# Go into the repository
cd electron-quick-start
# Install dependencies
npm install
# Run the app
npm start
</code></pre>
<p>安装好electron运行环境后，需要修改一下main.js配置：</p>
<pre><code class="language-js">function createWindow () {
  // Create the browser window.
  const mainWindow = new BrowserWindow({
    width: 800,
    height: 600,
    webPreferences: {
      preload: path.join(__dirname, 'preload.js'),
      nodeIntegration: true,
      enableRemoteModule: true,
      contextIsolation: false
    }
  })

  // and load the index.html of the app.
  // mainWindow.loadFile('./dist/index.html')
  # 本地项目调试地址
  mainWindow.loadURL(&quot;http://localhost:8080&quot;);
  // Open the DevTools.
  mainWindow.webContents.openDevTools()
}
</code></pre>
<h2><a id="%E9%97%AE%E9%A2%98" class="anchor" aria-hidden="true" href="#%E9%97%AE%E9%A2%98"><span class="octicon octicon-link"></span></a>问题</h2>
<h3><a id="uncaught-referenceerror-exports-is-not-defined%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95" class="anchor" aria-hidden="true" href="#uncaught-referenceerror-exports-is-not-defined%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="octicon octicon-link"></span></a>Uncaught ReferenceError: exports is not defined 解决办法</h3>
<p>这个问题是因为不支持ES6语法问题，需要将ES6转换成ES5。
转换方式：<a href="https://blog.csdn.net/weixin_43353035/article/details/126494522">https://blog.csdn.net/weixin_43353035/article/details/126494522</a></p>
<h3><a id="electron-require%E6%8A%A5%E9%94%99%EF%BC%9A-uncaught-referenceerror-require-is-not-defined" class="anchor" aria-hidden="true" href="#electron-require%E6%8A%A5%E9%94%99%EF%BC%9A-uncaught-referenceerror-require-is-not-defined"><span class="octicon octicon-link"></span></a>electron require()报错：Uncaught ReferenceError: require is not defined</h3>
<p>在election的main.js配置文件中加上这个配置即可：</p>
<pre><code>nodeIntegration: true, //渲染进程是否集成 Nodejs
contextIsolation: false,
enableRemoteModule: true //是否允许渲染进程使用远程模块
</code></pre>
<p>remote 使用：<a href="https://www.cnblogs.com/axl234/p/15206270.html">https://www.cnblogs.com/axl234/p/15206270.html</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[记一次Vue+electron实现前端调用fs模块操作系统文件]]></title>
    <link href="jythons.github.io/16869017655372.html"/>
    <updated>2023-06-16T15:49:25+08:00</updated>
    <id>jythons.github.io/16869017655372.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E5%9C%A8vue%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BC%80%E5%90%AFnode%E6%94%AF%E6%8C%81" class="anchor" aria-hidden="true" href="#%E5%9C%A8vue%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BC%80%E5%90%AFnode%E6%94%AF%E6%8C%81"><span class="octicon octicon-link"></span></a>在VUE配置中开启node支持</h2>
<pre><code>vue.config.js

const { defineConfig } = require('@vue/cli-service')
module.exports = defineConfig({
  transpileDependencies: true,
  pluginOptions:{
    electronBuilder:{
      nodeIntegration:true
    }
  }
})
</code></pre>
<pre><code>webpack.config.js

node: {
   fs: &quot;empty&quot;
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[golang类型]]></title>
    <link href="jythons.github.io/16849850924438.html"/>
    <updated>2023-05-25T11:24:52+08:00</updated>
    <id>jythons.github.io/16849850924438.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="chan" class="anchor" aria-hidden="true" href="#chan"><span class="octicon octicon-link"></span></a>chan</h2>
<p>通道类型常用于协成之间的通信，底层采用一个环形队列的方式实现了一个缓冲区，数据传输过来，如果等待队列里没有读的gorutine, 会将数据暂存到缓存去，如果队列满了，需要写入的协成会放到待写入队列阻塞，如果有读的协成进来会先从缓冲区读取一个数据，缓存区由两个游标控制，一个游标记录环形队列下一次要读取的位置，另一个游标记录环形队列下一次要存放的位置。
环形队列使用切片数组实现。通过游标位置与容量的取模，计算下一个游标位置。</p>
<p>select用于读取或写入channel，如果没有default case，select读取数据没有读取到，会阻塞，有default case，则不会阻塞，如果是读取数据，要判断是否读取成功，关闭的通道也可以读取，select每次获取通道数据时，会将case切片打乱进行读取，防止有的case被饿死。</p>
<h2><a id="%E5%88%87%E7%89%87" class="anchor" aria-hidden="true" href="#%E5%88%87%E7%89%87"><span class="octicon octicon-link"></span></a>切片</h2>
<p>切片类型是基于数组的，但是比数组灵活，可以动态扩容，切片在扩容之前，与数组共用一块内存空间，所以修改切片内元素，也会变更数组的元素，如果切片发生扩容，则会从新开辟一块内存空间，然后将原切片的内容拷贝过去，新写入的元素会写到新的切片后面，这时修改切片内容，不会影响原数组。切片可以使用数组创建，也可以使用make创建。通过数组创建切片，切片的容量是start到数组结尾。使用copy复制切片不会发生扩容。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[vue-element-admin 问题]]></title>
    <link href="jythons.github.io/16843986016050.html"/>
    <updated>2023-05-18T16:30:01+08:00</updated>
    <id>jythons.github.io/16843986016050.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="1%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5" class="anchor" aria-hidden="true" href="#1%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5"><span class="octicon octicon-link"></span></a>1.安装失败</h2>
<p>将python环境切换到2.X版本</p>
<h2><a id="2%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5" class="anchor" aria-hidden="true" href="#2%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5"><span class="octicon octicon-link"></span></a>2.启动失败</h2>
<p>1.克隆以后，删除package.json中tui-editor:1.3.3<br />
2.修改\src\components\MarkdownEditor\index.vue文件</p>
<p>将全部import换成下面几个</p>
<pre><code>import 'codemirror/lib/codemirror.css' 
import '@toast-ui/editor/dist/toastui-editor.css' 
import Editor from '@toast-ui/vue-editor' 
import defaultOptions from './default-options'  
</code></pre>
<p>并将该文件下的getValue和setValue分别换成getMarkdown和setMarkdown</p>
<p>3.单独安装tui-editor</p>
<pre><code>npm install --save @toast-ui/vue-editor
</code></pre>
<p>4.安装其他依赖</p>
<pre><code>npm i
</code></pre>
<p>5.跑起来</p>
<pre><code>npm run dev
</code></pre>
<blockquote>
<p>版权声明：本文为CSDN博主「阿莨去爬山了」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：<a href="https://blog.csdn.net/qq_44441509/article/details/128125578">https://blog.csdn.net/qq_44441509/article/details/128125578</a></p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GIT项目添加多个远程分支]]></title>
    <link href="jythons.github.io/16843800406124.html"/>
    <updated>2023-05-18T11:20:40+08:00</updated>
    <id>jythons.github.io/16843800406124.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<ol>
<li>添加一个远程分支</li>
</ol>
<pre><code>git remote add [远程分支名称] 地址
git remote add in http://abc%40sina.com@git.com
</code></pre>
<blockquote>
<p>注意，这里如果是http的Git地址，需要在远程分支地址前加上用户名，如果用户名包含‘@’，需要转换成‘%40’</p>
</blockquote>
<ol start="2">
<li>删除远程分支</li>
</ol>
<pre><code>git remote remove [远程分支名称]
</code></pre>
<ol start="3">
<li>拉取代码和提交代码</li>
</ol>
<pre><code>git pull [远程分支名称] [分支]
git push [远程分支名称] [分支]
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[vue post 下载文件]]></title>
    <link href="jythons.github.io/16823440770652.html"/>
    <updated>2023-04-24T21:47:57+08:00</updated>
    <id>jythons.github.io/16823440770652.html</id>
    <content type="html"><![CDATA[
<p>使用file-saver导出文件</p>
<span id="more"></span><!-- more -->
<h2><a id="%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85" class="anchor" aria-hidden="true" href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85"><span class="octicon octicon-link"></span></a>一、安装</h2>
<pre><code class="language-shell">npm install file-saver --save
# 如使用TS开发，可安装file-saver的TypeScript类型定义
npm install @types/file-saver --save-dev
</code></pre>
<h2><a id="%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8" class="anchor" aria-hidden="true" href="#%E4%BA%8C%E3%80%81%E4%BD%BF%E7%94%A8"><span class="octicon octicon-link"></span></a>二、使用</h2>
<pre><code class="language-js">import { saveAs } from 'file-saver'
// 保存文本
const blob = new Blob(['Hello, world!'])
saveAs(blob, 'hello world.txt')
// 预览图片
saveAs('https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png')
// 保存文件
const blob = new Blob([fileStream])// fileStream 是文件流，一般从后台获取
saveAs(blob, fileName)// fileName 保存文件的名称，需要带后缀
</code></pre>
<h2><a id="%E4%B8%89%E3%80%81%E9%80%9A%E7%94%A8%E5%9C%BA%E6%99%AF" class="anchor" aria-hidden="true" href="#%E4%B8%89%E3%80%81%E9%80%9A%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="octicon octicon-link"></span></a>三、通用场景</h2>
<p>一般开发过程中，需要下载本地文件，或者从服务器下载文件，可使用下面封装的方法。</p>
<h3><a id="%E4%B8%8B%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6" class="anchor" aria-hidden="true" href="#%E4%B8%8B%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6"><span class="octicon octicon-link"></span></a>下载本地文件</h3>
<pre><code class="language-js">	// file.js 封装下载本地文件方法
	import axios from 'axios'
	import { saveAs } from 'file-saver'
	/**
	 * @params {string} localFileName 本地文件名称
	 * @params {string} saveFileName 下载的文件名称
	 * @retuen {promise}
	 */
	export const downloadLocalFile = (localFileName, saveFileName) =&gt; {
	    return new Promise((resolve, reject) =&gt; {
	        axios({
	            url: `/file/${localFileName}`,	// 本地文件夹路径+本地文件名称(若资源在服务器，且是具体的路径，这里可改成该资源路径，此时封装的方法需要微调，入参的localFileName改成资源路径resource)
	            method: 'get',					
	            responseType: 'blob',			//	arraybuffer	也可
	        }).then(res =&gt; {
	            const blob = new Blob([res.data])
	            if (navigator.msSaveBlob) {			// 兼容IE
	                navigator.msSaveBlob(blob, saveFileName)
	            } else {
	                const url = window.URL.createObjectURL(blob)
	                saveAs(url, saveFileName)
	            }
	            resolve()
	        }).catch(err =&gt; {
	        	// 这里可以统一处理错误，比如&quot;未找到相关文件&quot;，&quot;下载失败&quot;等
	        	if (err.message === 'Request failed with status code 404') {
	        		// 提示or弹框：未找到相关文件
	        	} else {
	        		// 提示or弹框：下载失败
	        	}
	            reject(err)
	        })
	    })
	}

	// 使用（注意文件格式的后缀名）
	downloadLocalFile('excelFile.xlsx', 'newExcelFile.xlsx').then(res =&gt; {
		// 下载成功后的操作
		console.log('下载成功！')
	})

</code></pre>
<h3><a id="%E4%B8%8B%E8%BD%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%EF%BC%88%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%94%E5%9B%9E%E6%96%87%E4%BB%B6%E6%B5%81%EF%BC%89" class="anchor" aria-hidden="true" href="#%E4%B8%8B%E8%BD%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%EF%BC%88%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%94%E5%9B%9E%E6%96%87%E4%BB%B6%E6%B5%81%EF%BC%89"><span class="octicon octicon-link"></span></a>下载服务器文件（服务器返回文件流）</h3>
<pre><code class="language-js">	// file.js 封装下载本地文件方法
	import axios from 'axios'
	import { saveAs } from 'file-saver'
	/**
	 * @params {stream} fileStream 服务器返回的文件流
	 * @params {string} saveFileName 下载的文件名称
	 * @retuen {promise}
	 */
	 export const downloadFile = (fileStream, saveFileName) =&gt; {
		return new Promise((resolve, reject) =&gt; {
	 		const blob = new Blob([fileStream])
	 		if (navigator.msSaveBlob) {			// 兼容IE
	        	navigator.msSaveBlob(blob, saveFileName)
	       	} else {
	      		const url = window.URL.createObjectURL(blob)
	       		saveAs(url, saveFileName)
	       	}
	     	resolve()
	 	}) 
	 }
	 
	// 使用（注意文件格式的后缀名）
	const fileStream = await xxApi()  // 请求后台接口，获取文件流
	downloadFile(fileStream, 'file.pdf').then(res =&gt; {
		// 下载成功后的操作
		console.log('下载成功！')
	})

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL 时间问题]]></title>
    <link href="jythons.github.io/16802311282072.html"/>
    <updated>2023-03-31T10:52:08+08:00</updated>
    <id>jythons.github.io/16802311282072.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->
<h2><a id="%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E5%8C%BA" class="anchor" aria-hidden="true" href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E5%8C%BA"><span class="octicon octicon-link"></span></a>查看当前数据库时区</h2>
<pre><code>show variables like '%time_zone%';
</code></pre>
<h2><a id="%E4%BF%AE%E6%94%B9%E6%97%B6%E5%8C%BA" class="anchor" aria-hidden="true" href="#%E4%BF%AE%E6%94%B9%E6%97%B6%E5%8C%BA"><span class="octicon octicon-link"></span></a>修改时区</h2>
<h3><a id="%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BF%AE%E6%94%B9" class="anchor" aria-hidden="true" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BF%AE%E6%94%B9"><span class="octicon octicon-link"></span></a>命令行修改</h3>
<pre><code>set global time_zone = '+8:00';
flush privileges;
</code></pre>
<h3><a id="%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6etcmy-cnf" class="anchor" aria-hidden="true" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6etcmy-cnf"><span class="octicon octicon-link"></span></a>修改配置文件 /etc/my.cnf</h3>
<pre><code>[mysqld]
default-time_zone = '+8:00'
</code></pre>

]]></content>
  </entry>
  
</feed>
